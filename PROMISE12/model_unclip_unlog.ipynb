{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsun/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras import optimizers\n",
    "import keras.layers.advanced_activations\n",
    "import scipy\n",
    "import random\n",
    "import matplotlib.pyplot as plt  \n",
    "from scipy.stats import norm  \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import VarianceScaling,RandomNormal\n",
    "from keras.layers import Input, Dense, Dropout, Conv2D, MaxPooling2D, concatenate\n",
    "from keras.layers import UpSampling2D, Reshape, Lambda, Flatten, Activation,Concatenate\n",
    "from keras.models import Model  \n",
    "from keras.optimizers import SGD, Adadelta, Adagrad,Adam\n",
    "from keras import backend as K  \n",
    "from keras import objectives  \n",
    "from keras.utils.vis_utils import plot_model  \n",
    "from keras.utils import np_utils, generic_utils\n",
    "import sys \n",
    "import tensorflow as tf\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4,5'\n",
    "import sklearn\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import NullFormatter\n",
    "%matplotlib inline\n",
    "from sklearn import manifold, datasets,cluster\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = np.load(\"img_set_unclip_unlog.npy\").reshape([-1,256,256,1])\n",
    "train_seg = np.load(\"seg_set_unclip_unlog.npy\").reshape([-1,256,256,1])\n",
    "train_flo = np.load(\"flo_set_rgb_unclip_unlog.npy\")\n",
    "train_rflo = np.load(\"rflo_set_rgb_unclip_unlog.npy\")\n",
    "train_img = np.log(train_img+1.0)\n",
    "train_img = train_img/train_img.max()\n",
    "train_seg = np_utils.to_categorical(train_seg, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline: without optical flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols = 256, 256\n",
    "img_channels = 1\n",
    "\n",
    "batch_size =50\n",
    "latent_dim = 256\n",
    "nb_epoch = 50\n",
    "intermediate_dim =256\n",
    "original_dim = img_rows*img_cols\n",
    "LRelu = 'relu'\n",
    "\n",
    "#USE = 'autoencoder'\n",
    "#USE = 'vae'\n",
    "#encoder:\n",
    "\n",
    "input_img = Input(shape=(img_rows, img_cols,img_channels))\n",
    "\n",
    "conv_1 = Conv2D(20, (3, 3), padding='same',kernel_initializer='normal',dilation_rate=2)(input_img)\n",
    "conv_1 = Activation('relu')(conv_1)\n",
    "conv_1 = BatchNormalization()(conv_1)\n",
    "maxpool_1 = MaxPooling2D((2, 2),padding='same')(conv_1)\n",
    "#maxpool_1 = Dropout(0.3)(maxpool_1)\n",
    "\n",
    "\n",
    "conv_2 = Conv2D(20, (3, 3), padding='same',kernel_initializer='normal',dilation_rate=2)(maxpool_1)\n",
    "conv_2 = Activation('relu')(conv_2)\n",
    "#conv_2 = Dropout(0.3)(conv_2)\n",
    "conv_2 = BatchNormalization()(conv_2)\n",
    "maxpool_2 = MaxPooling2D((2, 2),  padding='same')(conv_2)\n",
    "#maxpool_2 = Dropout(0.3)(maxpool_2)\n",
    "\n",
    "conv_3 = Conv2D(20, (3, 3),padding='same',kernel_initializer='normal',dilation_rate=1)(maxpool_2)\n",
    "conv_3 = Activation(LRelu)(conv_3)\n",
    "#conv_3 = Dropout(0.3)(conv_3)\n",
    "conv_3 = BatchNormalization()(conv_3)\n",
    "maxpool_3 = MaxPooling2D((2, 2),  padding='same')(conv_3)\n",
    "#maxpool_3 = Dropout(0.3)(maxpool_3)\n",
    "\n",
    "conv_4 = Conv2D(20, (3, 3),padding='same',kernel_initializer='normal',dilation_rate=1)(maxpool_3)\n",
    "conv_4 = Activation(LRelu)(conv_4)\n",
    "#conv_4 = Dropout(0.3)(conv_4)\n",
    "conv_4 = BatchNormalization()(conv_4)\n",
    "maxpool_4 = MaxPooling2D((2, 2),  padding='same')(conv_4)\n",
    "#maxpool_4 = Dropout(0.3)(maxpool_4)\n",
    "\n",
    "#conv_5 = Conv2D(20, (3, 3), activation='relu', padding='same',kernel_initializer='normal')(maxpool_4)\n",
    "#maxpool_5 = MaxPooling2D((2, 2),  padding='same')(conv_5)\n",
    "\n",
    "\n",
    "#x = Conv2D(5, (3, 3), activation='relu', padding='same',kernel_initializer='normal')(x)\n",
    "#x = MaxPooling2D((2, 2),  padding='same')(x)\n",
    "\n",
    "visual = Flatten()(maxpool_4)\n",
    "h_1 = Dense(intermediate_dim, activation='relu')(visual)#relu?\n",
    "encoded = Dense(latent_dim, activation='tanh')(h_1)# relu?\n",
    "\n",
    "\n",
    "h_3 = Dense(intermediate_dim,activation=LRelu)(encoded)#for AE\n",
    "\n",
    "h_4 = Dense(20*16*16,activation=LRelu)(h_3)\n",
    "h_5 = Reshape((16,16,20))(h_4)\n",
    "\n",
    "\n",
    "#conv_6 = Conv2D(20, (3, 3), activation='relu', padding='same',kernel_initializer='normal')(h_5)\n",
    "#upsample_6 = UpSampling2D((2, 2))(conv_6)\n",
    "\n",
    "conv_7 = Conv2D(20, (3, 3), activation='relu', padding='same',kernel_initializer='normal')(h_5)\n",
    "upsample_7 = UpSampling2D((2, 2))(conv_7)\n",
    "\n",
    "upsample_7 = Concatenate()([upsample_7,conv_4])\n",
    "conv_8 = Conv2D(20, (3, 3), activation='relu', padding='same',kernel_initializer='normal')(upsample_7)\n",
    "upsample_8 = UpSampling2D((2, 2))(conv_8)\n",
    "\n",
    "upsample_8 = Concatenate()([upsample_8,conv_3])\n",
    "conv_9 = Conv2D(10, (3, 3), activation='relu', padding='same',kernel_initializer='normal')(upsample_8)\n",
    "upsample_9 = UpSampling2D((2, 2))(conv_9)\n",
    "\n",
    "upsample_9 = Concatenate()([upsample_9,conv_2])\n",
    "conv_10 = Conv2D(5,  (3, 3), activation='relu',padding='same',kernel_initializer='normal')(upsample_9)\n",
    "upsample_10 = UpSampling2D((2, 2))(conv_10)\n",
    "\n",
    "upsample_10 = Concatenate()([upsample_10,conv_1])\n",
    "decoded = Conv2D(2, (3, 3), activation='softmax', padding='same')(upsample_10)\n",
    "#decoded = Flatten()(decoded)\n",
    "#decoded = Dense(256*256,activation='softmax')(decoded)\n",
    "\n",
    "\n",
    "EarlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=0, mode='auto')\n",
    "checkpoint = ModelCheckpoint('model_keras0516GPU0.h5',monitor ='val_loss',verbose = 1,save_best_only = True)\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true[:,:,:,1]) \n",
    "    y_pred_f = K.flatten(y_pred[:,:,:,1]) \n",
    "    intersection = K.sum(y_true_f * y_pred_f) \n",
    "    return (2. * intersection + 1e-6) / (K.sum(K.square(y_true_f)) + K.sum(K.square(y_pred_f)) + 1e-6)\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "#def ae_loss(x, decoded):  \n",
    "#    xent_loss = original_dim * objectives.mean_squared_error(x,decoded)\n",
    "#    return xent_loss\n",
    "adam = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "autoencoder = Model(inputs=input_img, outputs=decoded)\n",
    "autoencoder.compile(optimizer=adam, loss=dice_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1150 samples, validate on 127 samples\n",
      "Epoch 1/200\n",
      "1150/1150 [==============================] - 8s 7ms/step - loss: -0.1608 - val_loss: -0.0550\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.05498, saving model to model_keras0516GPU0.h5\n",
      "Epoch 2/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.2217 - val_loss: -0.0856\n",
      "\n",
      "Epoch 00002: val_loss improved from -0.05498 to -0.08559, saving model to model_keras0516GPU0.h5\n",
      "Epoch 3/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.3306 - val_loss: -0.1789\n",
      "\n",
      "Epoch 00003: val_loss improved from -0.08559 to -0.17892, saving model to model_keras0516GPU0.h5\n",
      "Epoch 4/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.5262 - val_loss: -0.2790\n",
      "\n",
      "Epoch 00004: val_loss improved from -0.17892 to -0.27899, saving model to model_keras0516GPU0.h5\n",
      "Epoch 5/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.5868 - val_loss: -0.2036\n",
      "\n",
      "Epoch 00005: val_loss did not improve from -0.27899\n",
      "Epoch 6/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.6388 - val_loss: -0.2279\n",
      "\n",
      "Epoch 00006: val_loss did not improve from -0.27899\n",
      "Epoch 7/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.6949 - val_loss: -0.1415\n",
      "\n",
      "Epoch 00007: val_loss did not improve from -0.27899\n",
      "Epoch 8/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.7686 - val_loss: -0.2602\n",
      "\n",
      "Epoch 00008: val_loss did not improve from -0.27899\n",
      "Epoch 9/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.8282 - val_loss: -0.4068\n",
      "\n",
      "Epoch 00009: val_loss improved from -0.27899 to -0.40679, saving model to model_keras0516GPU0.h5\n",
      "Epoch 10/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.8621 - val_loss: -0.5154\n",
      "\n",
      "Epoch 00010: val_loss improved from -0.40679 to -0.51539, saving model to model_keras0516GPU0.h5\n",
      "Epoch 11/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.8780 - val_loss: -0.6415\n",
      "\n",
      "Epoch 00011: val_loss improved from -0.51539 to -0.64151, saving model to model_keras0516GPU0.h5\n",
      "Epoch 12/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.8949 - val_loss: -0.6231\n",
      "\n",
      "Epoch 00012: val_loss did not improve from -0.64151\n",
      "Epoch 13/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9084 - val_loss: -0.5598\n",
      "\n",
      "Epoch 00013: val_loss did not improve from -0.64151\n",
      "Epoch 14/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9128 - val_loss: -0.6842\n",
      "\n",
      "Epoch 00014: val_loss improved from -0.64151 to -0.68424, saving model to model_keras0516GPU0.h5\n",
      "Epoch 15/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9178 - val_loss: -0.7254\n",
      "\n",
      "Epoch 00015: val_loss improved from -0.68424 to -0.72537, saving model to model_keras0516GPU0.h5\n",
      "Epoch 16/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9216 - val_loss: -0.7363\n",
      "\n",
      "Epoch 00016: val_loss improved from -0.72537 to -0.73631, saving model to model_keras0516GPU0.h5\n",
      "Epoch 17/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9271 - val_loss: -0.7057\n",
      "\n",
      "Epoch 00017: val_loss did not improve from -0.73631\n",
      "Epoch 18/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9325 - val_loss: -0.7514\n",
      "\n",
      "Epoch 00018: val_loss improved from -0.73631 to -0.75138, saving model to model_keras0516GPU0.h5\n",
      "Epoch 19/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9357 - val_loss: -0.7355\n",
      "\n",
      "Epoch 00019: val_loss did not improve from -0.75138\n",
      "Epoch 20/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9403 - val_loss: -0.7362\n",
      "\n",
      "Epoch 00020: val_loss did not improve from -0.75138\n",
      "Epoch 21/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9432 - val_loss: -0.7414\n",
      "\n",
      "Epoch 00021: val_loss did not improve from -0.75138\n",
      "Epoch 22/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9450 - val_loss: -0.7348\n",
      "\n",
      "Epoch 00022: val_loss did not improve from -0.75138\n",
      "Epoch 23/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9448 - val_loss: -0.7236\n",
      "\n",
      "Epoch 00023: val_loss did not improve from -0.75138\n",
      "Epoch 24/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9473 - val_loss: -0.7403\n",
      "\n",
      "Epoch 00024: val_loss did not improve from -0.75138\n",
      "Epoch 25/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9492 - val_loss: -0.7352\n",
      "\n",
      "Epoch 00025: val_loss did not improve from -0.75138\n",
      "Epoch 26/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9515 - val_loss: -0.7350\n",
      "\n",
      "Epoch 00026: val_loss did not improve from -0.75138\n",
      "Epoch 27/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9527 - val_loss: -0.7474\n",
      "\n",
      "Epoch 00027: val_loss did not improve from -0.75138\n",
      "Epoch 28/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9536 - val_loss: -0.7446\n",
      "\n",
      "Epoch 00028: val_loss did not improve from -0.75138\n",
      "Epoch 29/200\n",
      "1150/1150 [==============================] - 5s 5ms/step - loss: -0.9528 - val_loss: -0.7330\n",
      "\n",
      "Epoch 00029: val_loss did not improve from -0.75138\n",
      "Epoch 30/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9536 - val_loss: -0.7499\n",
      "\n",
      "Epoch 00030: val_loss did not improve from -0.75138\n",
      "Epoch 31/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9575 - val_loss: -0.7376\n",
      "\n",
      "Epoch 00031: val_loss did not improve from -0.75138\n",
      "Epoch 32/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9603 - val_loss: -0.7432\n",
      "\n",
      "Epoch 00032: val_loss did not improve from -0.75138\n",
      "Epoch 33/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9613 - val_loss: -0.7380\n",
      "\n",
      "Epoch 00033: val_loss did not improve from -0.75138\n",
      "Epoch 34/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9603 - val_loss: -0.7142\n",
      "\n",
      "Epoch 00034: val_loss did not improve from -0.75138\n",
      "Epoch 35/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9612 - val_loss: -0.7425\n",
      "\n",
      "Epoch 00035: val_loss did not improve from -0.75138\n",
      "Epoch 36/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9625 - val_loss: -0.7445\n",
      "\n",
      "Epoch 00036: val_loss did not improve from -0.75138\n",
      "Epoch 37/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9643 - val_loss: -0.7401\n",
      "\n",
      "Epoch 00037: val_loss did not improve from -0.75138\n",
      "Epoch 38/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9647 - val_loss: -0.7111\n",
      "\n",
      "Epoch 00038: val_loss did not improve from -0.75138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feffea5da20>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CheckPoint = keras.callbacks.ModelCheckpoint(\"Model_keras.h5\", monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "autoencoder.fit(train_img[:1150], train_seg[:1150],\n",
    "        shuffle=True,\n",
    "        epochs=200,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(train_img[1150:],train_seg[1150:]),callbacks=[EarlyStopping,checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# then use optical flow, concate 到最后一层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols = 256, 256\n",
    "img_channels = 1\n",
    "\n",
    "batch_size =50\n",
    "latent_dim = 256\n",
    "nb_epoch = 50\n",
    "intermediate_dim =256\n",
    "original_dim = img_rows*img_cols\n",
    "LRelu = 'relu'\n",
    "\n",
    "#USE = 'autoencoder'\n",
    "#USE = 'vae'\n",
    "#encoder:\n",
    "\n",
    "input_img = Input(shape=(img_rows, img_cols,img_channels))\n",
    "input_flo = Input(shape=(img_rows, img_cols,3))\n",
    "input_rflo = Input(shape=(img_rows, img_cols,3))\n",
    "\n",
    "conv_1 = Conv2D(20, (3, 3), padding='same',kernel_initializer='normal',dilation_rate=2)(input_img)\n",
    "conv_1 = Activation('relu')(conv_1)\n",
    "conv_1 = BatchNormalization()(conv_1)\n",
    "maxpool_1 = MaxPooling2D((2, 2),padding='same')(conv_1)\n",
    "#maxpool_1 = Dropout(0.3)(maxpool_1)\n",
    "\n",
    "\n",
    "conv_2 = Conv2D(20, (3, 3), padding='same',kernel_initializer='normal',dilation_rate=2)(maxpool_1)\n",
    "conv_2 = Activation('relu')(conv_2)\n",
    "#conv_2 = Dropout(0.3)(conv_2)\n",
    "conv_2 = BatchNormalization()(conv_2)\n",
    "maxpool_2 = MaxPooling2D((2, 2),  padding='same')(conv_2)\n",
    "#maxpool_2 = Dropout(0.3)(maxpool_2)\n",
    "\n",
    "conv_3 = Conv2D(20, (3, 3),padding='same',kernel_initializer='normal',dilation_rate=1)(maxpool_2)\n",
    "conv_3 = Activation(LRelu)(conv_3)\n",
    "#conv_3 = Dropout(0.3)(conv_3)\n",
    "conv_3 = BatchNormalization()(conv_3)\n",
    "maxpool_3 = MaxPooling2D((2, 2),  padding='same')(conv_3)\n",
    "#maxpool_3 = Dropout(0.3)(maxpool_3)\n",
    "\n",
    "conv_4 = Conv2D(20, (3, 3),padding='same',kernel_initializer='normal',dilation_rate=1)(maxpool_3)\n",
    "conv_4 = Activation(LRelu)(conv_4)\n",
    "#conv_4 = Dropout(0.3)(conv_4)\n",
    "conv_4 = BatchNormalization()(conv_4)\n",
    "maxpool_4 = MaxPooling2D((2, 2),  padding='same')(conv_4)\n",
    "#maxpool_4 = Dropout(0.3)(maxpool_4)\n",
    "\n",
    "#conv_5 = Conv2D(20, (3, 3), activation='relu', padding='same',kernel_initializer='normal')(maxpool_4)\n",
    "#maxpool_5 = MaxPooling2D((2, 2),  padding='same')(conv_5)\n",
    "\n",
    "\n",
    "#x = Conv2D(5, (3, 3), activation='relu', padding='same',kernel_initializer='normal')(x)\n",
    "#x = MaxPooling2D((2, 2),  padding='same')(x)\n",
    "\n",
    "visual = Flatten()(maxpool_4)\n",
    "h_1 = Dense(intermediate_dim, activation='relu')(visual)#relu?\n",
    "encoded = Dense(latent_dim, activation='tanh')(h_1)# relu?\n",
    "\n",
    "\n",
    "h_3 = Dense(intermediate_dim,activation=LRelu)(encoded)#for AE\n",
    "\n",
    "h_4 = Dense(20*16*16,activation=LRelu)(h_3)\n",
    "h_5 = Reshape((16,16,20))(h_4)\n",
    "\n",
    "\n",
    "#conv_6 = Conv2D(20, (3, 3), activation='relu', padding='same',kernel_initializer='normal')(h_5)\n",
    "#upsample_6 = UpSampling2D((2, 2))(conv_6)\n",
    "\n",
    "conv_7 = Conv2D(20, (3, 3), activation='relu', padding='same',kernel_initializer='normal')(h_5)\n",
    "upsample_7 = UpSampling2D((2, 2))(conv_7)\n",
    "\n",
    "upsample_7 = Concatenate()([upsample_7,conv_4])\n",
    "conv_8 = Conv2D(20, (3, 3), activation='relu', padding='same',kernel_initializer='normal')(upsample_7)\n",
    "upsample_8 = UpSampling2D((2, 2))(conv_8)\n",
    "\n",
    "upsample_8 = Concatenate()([upsample_8,conv_3])\n",
    "conv_9 = Conv2D(10, (3, 3), activation='relu', padding='same',kernel_initializer='normal')(upsample_8)\n",
    "upsample_9 = UpSampling2D((2, 2))(conv_9)\n",
    "\n",
    "upsample_9 = Concatenate()([upsample_9,conv_2])\n",
    "conv_10 = Conv2D(5,  (3, 3), activation='relu',padding='same',kernel_initializer='normal')(upsample_9)\n",
    "upsample_10 = UpSampling2D((2, 2))(conv_10)\n",
    "\n",
    "upsample_10 = Concatenate()([upsample_10,conv_1,input_flo,input_rflo])\n",
    "decoded = Conv2D(2, (3, 3), activation='softmax', padding='same')(upsample_10)\n",
    "#decoded = Flatten()(decoded)\n",
    "#decoded = Dense(256*256,activation='softmax')(decoded)\n",
    "\n",
    "\n",
    "EarlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=0, mode='auto')\n",
    "checkpoint = ModelCheckpoint('model_keras0516GPU0.h5',monitor ='val_loss',verbose = 1,save_best_only = True)\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true[:,:,:,1]) \n",
    "    y_pred_f = K.flatten(y_pred[:,:,:,1]) \n",
    "    intersection = K.sum(y_true_f * y_pred_f) \n",
    "    return (2. * intersection + 1e-6) / (K.sum(K.square(y_true_f)) + K.sum(K.square(y_pred_f)) + 1e-6)\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "#def ae_loss(x, decoded):  \n",
    "#    xent_loss = original_dim * objectives.mean_squared_error(x,decoded)\n",
    "#    return xent_loss\n",
    "adam = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "autoencoder = Model(inputs=[input_img,input_flo,input_rflo], outputs=decoded)\n",
    "autoencoder.compile(optimizer=adam, loss=dice_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1150 samples, validate on 127 samples\n",
      "Epoch 1/200\n",
      "1150/1150 [==============================] - 11s 10ms/step - loss: -0.1514 - val_loss: -0.0521\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.05208, saving model to model_keras0516GPU0.h5\n",
      "Epoch 2/200\n",
      "1150/1150 [==============================] - 8s 7ms/step - loss: -0.1654 - val_loss: -0.0571\n",
      "\n",
      "Epoch 00002: val_loss improved from -0.05208 to -0.05706, saving model to model_keras0516GPU0.h5\n",
      "Epoch 3/200\n",
      "1150/1150 [==============================] - 7s 6ms/step - loss: -0.1841 - val_loss: -0.0663\n",
      "\n",
      "Epoch 00003: val_loss improved from -0.05706 to -0.06634, saving model to model_keras0516GPU0.h5\n",
      "Epoch 4/200\n",
      "1150/1150 [==============================] - 7s 6ms/step - loss: -0.2183 - val_loss: -0.0993\n",
      "\n",
      "Epoch 00004: val_loss improved from -0.06634 to -0.09927, saving model to model_keras0516GPU0.h5\n",
      "Epoch 5/200\n",
      "1150/1150 [==============================] - 7s 6ms/step - loss: -0.3215 - val_loss: -0.3509\n",
      "\n",
      "Epoch 00005: val_loss improved from -0.09927 to -0.35086, saving model to model_keras0516GPU0.h5\n",
      "Epoch 6/200\n",
      "1150/1150 [==============================] - 7s 6ms/step - loss: -0.5603 - val_loss: -0.4507\n",
      "\n",
      "Epoch 00006: val_loss improved from -0.35086 to -0.45071, saving model to model_keras0516GPU0.h5\n",
      "Epoch 7/200\n",
      "1150/1150 [==============================] - 7s 6ms/step - loss: -0.6209 - val_loss: -0.3304\n",
      "\n",
      "Epoch 00007: val_loss did not improve from -0.45071\n",
      "Epoch 8/200\n",
      "1150/1150 [==============================] - 7s 6ms/step - loss: -0.6656 - val_loss: -0.2543\n",
      "\n",
      "Epoch 00008: val_loss did not improve from -0.45071\n",
      "Epoch 9/200\n",
      "1150/1150 [==============================] - 7s 6ms/step - loss: -0.7492 - val_loss: -0.4237\n",
      "\n",
      "Epoch 00009: val_loss did not improve from -0.45071\n",
      "Epoch 10/200\n",
      "1150/1150 [==============================] - 7s 7ms/step - loss: -0.8230 - val_loss: -0.6446\n",
      "\n",
      "Epoch 00010: val_loss improved from -0.45071 to -0.64460, saving model to model_keras0516GPU0.h5\n",
      "Epoch 11/200\n",
      "1150/1150 [==============================] - 7s 6ms/step - loss: -0.8608 - val_loss: -0.6157\n",
      "\n",
      "Epoch 00011: val_loss did not improve from -0.64460\n",
      "Epoch 12/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.8844 - val_loss: -0.6217\n",
      "\n",
      "Epoch 00012: val_loss did not improve from -0.64460\n",
      "Epoch 13/200\n",
      "1150/1150 [==============================] - 7s 6ms/step - loss: -0.8953 - val_loss: -0.6643\n",
      "\n",
      "Epoch 00013: val_loss improved from -0.64460 to -0.66428, saving model to model_keras0516GPU0.h5\n",
      "Epoch 14/200\n",
      "1150/1150 [==============================] - 6s 6ms/step - loss: -0.9092 - val_loss: -0.6827\n",
      "\n",
      "Epoch 00014: val_loss improved from -0.66428 to -0.68271, saving model to model_keras0516GPU0.h5\n",
      "Epoch 15/200\n",
      "1150/1150 [==============================] - 6s 6ms/step - loss: -0.9174 - val_loss: -0.7188\n",
      "\n",
      "Epoch 00015: val_loss improved from -0.68271 to -0.71880, saving model to model_keras0516GPU0.h5\n",
      "Epoch 16/200\n",
      "1150/1150 [==============================] - 6s 6ms/step - loss: -0.9244 - val_loss: -0.7244\n",
      "\n",
      "Epoch 00016: val_loss improved from -0.71880 to -0.72441, saving model to model_keras0516GPU0.h5\n",
      "Epoch 17/200\n",
      "1150/1150 [==============================] - 7s 6ms/step - loss: -0.9283 - val_loss: -0.7229\n",
      "\n",
      "Epoch 00017: val_loss did not improve from -0.72441\n",
      "Epoch 18/200\n",
      "1150/1150 [==============================] - 7s 6ms/step - loss: -0.9310 - val_loss: -0.7332\n",
      "\n",
      "Epoch 00018: val_loss improved from -0.72441 to -0.73325, saving model to model_keras0516GPU0.h5\n",
      "Epoch 19/200\n",
      "1150/1150 [==============================] - 6s 6ms/step - loss: -0.9362 - val_loss: -0.7407\n",
      "\n",
      "Epoch 00019: val_loss improved from -0.73325 to -0.74074, saving model to model_keras0516GPU0.h5\n",
      "Epoch 20/200\n",
      "1150/1150 [==============================] - 7s 6ms/step - loss: -0.9397 - val_loss: -0.7323\n",
      "\n",
      "Epoch 00020: val_loss did not improve from -0.74074\n",
      "Epoch 21/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9412 - val_loss: -0.7280\n",
      "\n",
      "Epoch 00021: val_loss did not improve from -0.74074\n",
      "Epoch 22/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9444 - val_loss: -0.7216\n",
      "\n",
      "Epoch 00022: val_loss did not improve from -0.74074\n",
      "Epoch 23/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9465 - val_loss: -0.7396\n",
      "\n",
      "Epoch 00023: val_loss did not improve from -0.74074\n",
      "Epoch 24/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9481 - val_loss: -0.7353\n",
      "\n",
      "Epoch 00024: val_loss did not improve from -0.74074\n",
      "Epoch 25/200\n",
      "1150/1150 [==============================] - 7s 6ms/step - loss: -0.9491 - val_loss: -0.7311\n",
      "\n",
      "Epoch 00025: val_loss did not improve from -0.74074\n",
      "Epoch 26/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9507 - val_loss: -0.7314\n",
      "\n",
      "Epoch 00026: val_loss did not improve from -0.74074\n",
      "Epoch 27/200\n",
      "1150/1150 [==============================] - 6s 6ms/step - loss: -0.9520 - val_loss: -0.7578\n",
      "\n",
      "Epoch 00027: val_loss improved from -0.74074 to -0.75780, saving model to model_keras0516GPU0.h5\n",
      "Epoch 28/200\n",
      "1150/1150 [==============================] - 7s 6ms/step - loss: -0.9534 - val_loss: -0.7491\n",
      "\n",
      "Epoch 00028: val_loss did not improve from -0.75780\n",
      "Epoch 29/200\n",
      "1150/1150 [==============================] - 7s 6ms/step - loss: -0.9530 - val_loss: -0.7413\n",
      "\n",
      "Epoch 00029: val_loss did not improve from -0.75780\n",
      "Epoch 30/200\n",
      "1150/1150 [==============================] - 7s 6ms/step - loss: -0.9522 - val_loss: -0.7169\n",
      "\n",
      "Epoch 00030: val_loss did not improve from -0.75780\n",
      "Epoch 31/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9560 - val_loss: -0.7384\n",
      "\n",
      "Epoch 00031: val_loss did not improve from -0.75780\n",
      "Epoch 32/200\n",
      "1150/1150 [==============================] - 7s 6ms/step - loss: -0.9582 - val_loss: -0.7323\n",
      "\n",
      "Epoch 00032: val_loss did not improve from -0.75780\n",
      "Epoch 33/200\n",
      "1150/1150 [==============================] - 7s 6ms/step - loss: -0.9574 - val_loss: -0.7467\n",
      "\n",
      "Epoch 00033: val_loss did not improve from -0.75780\n",
      "Epoch 34/200\n",
      "1150/1150 [==============================] - 7s 6ms/step - loss: -0.9595 - val_loss: -0.7386\n",
      "\n",
      "Epoch 00034: val_loss did not improve from -0.75780\n",
      "Epoch 35/200\n",
      "1150/1150 [==============================] - 7s 6ms/step - loss: -0.9603 - val_loss: -0.7383\n",
      "\n",
      "Epoch 00035: val_loss did not improve from -0.75780\n",
      "Epoch 36/200\n",
      "1150/1150 [==============================] - 7s 6ms/step - loss: -0.9611 - val_loss: -0.7193\n",
      "\n",
      "Epoch 00036: val_loss did not improve from -0.75780\n",
      "Epoch 37/200\n",
      "1150/1150 [==============================] - 7s 6ms/step - loss: -0.9622 - val_loss: -0.7480\n",
      "\n",
      "Epoch 00037: val_loss did not improve from -0.75780\n",
      "Epoch 38/200\n",
      "1150/1150 [==============================] - 7s 6ms/step - loss: -0.9627 - val_loss: -0.7394\n",
      "\n",
      "Epoch 00038: val_loss did not improve from -0.75780\n",
      "Epoch 39/200\n",
      "1150/1150 [==============================] - 7s 6ms/step - loss: -0.9633 - val_loss: -0.7412\n",
      "\n",
      "Epoch 00039: val_loss did not improve from -0.75780\n",
      "Epoch 40/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9632 - val_loss: -0.7342\n",
      "\n",
      "Epoch 00040: val_loss did not improve from -0.75780\n",
      "Epoch 41/200\n",
      "1150/1150 [==============================] - 6s 6ms/step - loss: -0.9631 - val_loss: -0.7291\n",
      "\n",
      "Epoch 00041: val_loss did not improve from -0.75780\n",
      "Epoch 42/200\n",
      "1150/1150 [==============================] - 6s 6ms/step - loss: -0.9646 - val_loss: -0.7135\n",
      "\n",
      "Epoch 00042: val_loss did not improve from -0.75780\n",
      "Epoch 43/200\n",
      "1150/1150 [==============================] - 7s 6ms/step - loss: -0.9665 - val_loss: -0.7333\n",
      "\n",
      "Epoch 00043: val_loss did not improve from -0.75780\n",
      "Epoch 44/200\n",
      "1150/1150 [==============================] - 7s 6ms/step - loss: -0.9673 - val_loss: -0.7172\n",
      "\n",
      "Epoch 00044: val_loss did not improve from -0.75780\n",
      "Epoch 45/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9680 - val_loss: -0.7256\n",
      "\n",
      "Epoch 00045: val_loss did not improve from -0.75780\n",
      "Epoch 46/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9681 - val_loss: -0.7362\n",
      "\n",
      "Epoch 00046: val_loss did not improve from -0.75780\n",
      "Epoch 47/200\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: -0.9682 - val_loss: -0.7396\n",
      "\n",
      "Epoch 00047: val_loss did not improve from -0.75780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff00056cbe0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CheckPoint = keras.callbacks.ModelCheckpoint(\"Model_keras.h5\", monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "autoencoder.fit([train_img[:1150],train_flo[:1150],train_rflo[:1150]], train_seg[:1150],\n",
    "        shuffle=True,\n",
    "        epochs=200,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=([train_img[1150:],train_flo[1150:],train_rflo[1150:]],train_seg[1150:]),callbacks=[EarlyStopping,checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mid layer concate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols = 256, 256\n",
    "img_channels = 1\n",
    "\n",
    "batch_size =50\n",
    "latent_dim = 256\n",
    "nb_epoch = 50\n",
    "intermediate_dim =256\n",
    "original_dim = img_rows*img_cols\n",
    "LRelu = 'relu'\n",
    "\n",
    "#USE = 'autoencoder'\n",
    "#USE = 'vae'\n",
    "#encoder:\n",
    "\n",
    "input_img = Input(shape=(img_rows, img_cols,img_channels))\n",
    "input_flo = Input(shape=(img_rows, img_cols,3))\n",
    "input_rflo = Input(shape=(img_rows, img_cols,3))\n",
    "\n",
    "concate_flo = Concatenate()([input_flo,input_rflo])\n",
    "conv_flo_1 = Conv2D(20, (3, 3), padding='same',kernel_initializer='normal',dilation_rate=2)(concate_flo)\n",
    "conv_flo_1 = Activation('relu')(conv_flo_1)\n",
    "conv_flo_1 = BatchNormalization()(conv_flo_1)\n",
    "maxpool_flo_1 = MaxPooling2D((2, 2),padding='same')(conv_flo_1)\n",
    "conv_flo_2 = Conv2D(20, (3, 3), padding='same',kernel_initializer='normal',dilation_rate=2)(maxpool_flo_1)\n",
    "conv_flo_2 = Activation('relu')(conv_flo_2)\n",
    "conv_flo_2 = BatchNormalization()(conv_flo_2)\n",
    "maxpool_flo_2 = MaxPooling2D((2, 2),  padding='same')(conv_flo_2)\n",
    "conv_flo_3 = Conv2D(20, (3, 3),padding='same',kernel_initializer='normal',dilation_rate=1)(maxpool_flo_2)\n",
    "conv_flo_3 = Activation(LRelu)(conv_flo_3)\n",
    "conv_flo_3 = BatchNormalization()(conv_flo_3)\n",
    "maxpool_flo_3 = MaxPooling2D((2, 2),  padding='same')(conv_flo_3)\n",
    "conv_flo_4 = Conv2D(20, (3, 3),padding='same',kernel_initializer='normal',dilation_rate=1)(maxpool_flo_3)\n",
    "conv_flo_4 = Activation(LRelu)(conv_flo_4)\n",
    "conv_flo_4 = BatchNormalization()(conv_flo_4)\n",
    "maxpool_flo_4 = MaxPooling2D((2, 2),  padding='same')(conv_flo_4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "conv_1 = Conv2D(20, (3, 3), padding='same',kernel_initializer='normal',dilation_rate=2)(input_img)\n",
    "conv_1 = Activation('relu')(conv_1)\n",
    "conv_1 = BatchNormalization()(conv_1)\n",
    "maxpool_1 = MaxPooling2D((2, 2),padding='same')(conv_1)\n",
    "#maxpool_1 = Dropout(0.3)(maxpool_1)\n",
    "\n",
    "conv_2 = Conv2D(20, (3, 3), padding='same',kernel_initializer='normal',dilation_rate=2)(maxpool_1)\n",
    "conv_2 = Activation('relu')(conv_2)\n",
    "#conv_2 = Dropout(0.3)(conv_2)\n",
    "conv_2 = BatchNormalization()(conv_2)\n",
    "maxpool_2 = MaxPooling2D((2, 2),  padding='same')(conv_2)\n",
    "#maxpool_2 = Dropout(0.3)(maxpool_2)\n",
    "\n",
    "conv_3 = Conv2D(20, (3, 3),padding='same',kernel_initializer='normal',dilation_rate=1)(maxpool_2)\n",
    "conv_3 = Activation(LRelu)(conv_3)\n",
    "#conv_3 = Dropout(0.3)(conv_3)\n",
    "conv_3 = BatchNormalization()(conv_3)\n",
    "maxpool_3 = MaxPooling2D((2, 2),  padding='same')(conv_3)\n",
    "#maxpool_3 = Dropout(0.3)(maxpool_3)\n",
    "\n",
    "conv_4 = Conv2D(20, (3, 3),padding='same',kernel_initializer='normal',dilation_rate=1)(maxpool_3)\n",
    "conv_4 = Activation(LRelu)(conv_4)\n",
    "#conv_4 = Dropout(0.3)(conv_4)\n",
    "conv_4 = BatchNormalization()(conv_4)\n",
    "maxpool_4 = MaxPooling2D((2, 2),  padding='same')(conv_4)\n",
    "#maxpool_4 = Dropout(0.3)(maxpool_4)\n",
    "\n",
    "#conv_5 = Conv2D(20, (3, 3), activation='relu', padding='same',kernel_initializer='normal')(maxpool_4)\n",
    "#maxpool_5 = MaxPooling2D((2, 2),  padding='same')(conv_5)\n",
    "\n",
    "\n",
    "#x = Conv2D(5, (3, 3), activation='relu', padding='same',kernel_initializer='normal')(x)\n",
    "#x = MaxPooling2D((2, 2),  padding='same')(x)\n",
    "\n",
    "maxpool_4 = Concatenate()([maxpool_flo_4,maxpool_4])\n",
    "visual = Flatten()(maxpool_4)\n",
    "h_1 = Dense(intermediate_dim, activation='relu')(visual)#relu?\n",
    "encoded = Dense(latent_dim, activation='tanh')(h_1)# relu?\n",
    "\n",
    "\n",
    "h_3 = Dense(intermediate_dim,activation=LRelu)(encoded)#for AE\n",
    "\n",
    "h_4 = Dense(20*16*16,activation=LRelu)(h_3)\n",
    "h_5 = Reshape((16,16,20))(h_4)\n",
    "\n",
    "\n",
    "#conv_6 = Conv2D(20, (3, 3), activation='relu', padding='same',kernel_initializer='normal')(h_5)\n",
    "#upsample_6 = UpSampling2D((2, 2))(conv_6)\n",
    "\n",
    "conv_7 = Conv2D(20, (3, 3), activation='relu', padding='same',kernel_initializer='normal')(h_5)\n",
    "upsample_7 = UpSampling2D((2, 2))(conv_7)\n",
    "\n",
    "upsample_7 = Concatenate()([upsample_7,conv_4])\n",
    "conv_8 = Conv2D(20, (3, 3), activation='relu', padding='same',kernel_initializer='normal')(upsample_7)\n",
    "upsample_8 = UpSampling2D((2, 2))(conv_8)\n",
    "\n",
    "upsample_8 = Concatenate()([upsample_8,conv_3])\n",
    "conv_9 = Conv2D(10, (3, 3), activation='relu', padding='same',kernel_initializer='normal')(upsample_8)\n",
    "upsample_9 = UpSampling2D((2, 2))(conv_9)\n",
    "\n",
    "upsample_9 = Concatenate()([upsample_9,conv_2])\n",
    "conv_10 = Conv2D(5,  (3, 3), activation='relu',padding='same',kernel_initializer='normal')(upsample_9)\n",
    "upsample_10 = UpSampling2D((2, 2))(conv_10)\n",
    "\n",
    "upsample_10 = Concatenate()([upsample_10,conv_1,input_flo,input_rflo])\n",
    "decoded = Conv2D(2, (3, 3), activation='softmax', padding='same')(upsample_10)\n",
    "#decoded = Flatten()(decoded)\n",
    "#decoded = Dense(256*256,activation='softmax')(decoded)\n",
    "\n",
    "\n",
    "EarlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=0, mode='auto')\n",
    "checkpoint = ModelCheckpoint('model_keras0516GPU0.h5',monitor ='val_loss',verbose = 1,save_best_only = True)\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true[:,:,:,1]) \n",
    "    y_pred_f = K.flatten(y_pred[:,:,:,1]) \n",
    "    intersection = K.sum(y_true_f * y_pred_f) \n",
    "    return (2. * intersection + 1e-6) / (K.sum(K.square(y_true_f)) + K.sum(K.square(y_pred_f)) + 1e-6)\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "#def ae_loss(x, decoded):  \n",
    "#    xent_loss = original_dim * objectives.mean_squared_error(x,decoded)\n",
    "#    return xent_loss\n",
    "adam = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "autoencoder = Model(inputs=[input_img,input_flo,input_rflo], outputs=decoded)\n",
    "autoencoder.compile(optimizer=adam, loss=dice_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1150 samples, validate on 127 samples\n",
      "Epoch 1/200\n",
      "1150/1150 [==============================] - 13s 11ms/step - loss: -0.2390 - val_loss: -0.1260\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.12595, saving model to model_keras0516GPU0.h5\n",
      "Epoch 2/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.2941 - val_loss: -0.2423\n",
      "\n",
      "Epoch 00002: val_loss improved from -0.12595 to -0.24228, saving model to model_keras0516GPU0.h5\n",
      "Epoch 3/200\n",
      "1150/1150 [==============================] - 10s 9ms/step - loss: -0.4342 - val_loss: -0.4294\n",
      "\n",
      "Epoch 00003: val_loss improved from -0.24228 to -0.42942, saving model to model_keras0516GPU0.h5\n",
      "Epoch 4/200\n",
      "1150/1150 [==============================] - 11s 9ms/step - loss: -0.5449 - val_loss: -0.4567\n",
      "\n",
      "Epoch 00004: val_loss improved from -0.42942 to -0.45674, saving model to model_keras0516GPU0.h5\n",
      "Epoch 5/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.6201 - val_loss: -0.5090\n",
      "\n",
      "Epoch 00005: val_loss improved from -0.45674 to -0.50902, saving model to model_keras0516GPU0.h5\n",
      "Epoch 6/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.6903 - val_loss: -0.4349\n",
      "\n",
      "Epoch 00006: val_loss did not improve from -0.50902\n",
      "Epoch 7/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.7600 - val_loss: -0.3829\n",
      "\n",
      "Epoch 00007: val_loss did not improve from -0.50902\n",
      "Epoch 8/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.8152 - val_loss: -0.3952\n",
      "\n",
      "Epoch 00008: val_loss did not improve from -0.50902\n",
      "Epoch 9/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.8466 - val_loss: -0.5275\n",
      "\n",
      "Epoch 00009: val_loss improved from -0.50902 to -0.52751, saving model to model_keras0516GPU0.h5\n",
      "Epoch 10/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.8730 - val_loss: -0.6077\n",
      "\n",
      "Epoch 00010: val_loss improved from -0.52751 to -0.60767, saving model to model_keras0516GPU0.h5\n",
      "Epoch 11/200\n",
      "1150/1150 [==============================] - 10s 9ms/step - loss: -0.8927 - val_loss: -0.6332\n",
      "\n",
      "Epoch 00011: val_loss improved from -0.60767 to -0.63318, saving model to model_keras0516GPU0.h5\n",
      "Epoch 12/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9061 - val_loss: -0.6800\n",
      "\n",
      "Epoch 00012: val_loss improved from -0.63318 to -0.67997, saving model to model_keras0516GPU0.h5\n",
      "Epoch 13/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9160 - val_loss: -0.6665\n",
      "\n",
      "Epoch 00013: val_loss did not improve from -0.67997\n",
      "Epoch 14/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9227 - val_loss: -0.6710\n",
      "\n",
      "Epoch 00014: val_loss did not improve from -0.67997\n",
      "Epoch 15/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9262 - val_loss: -0.6857\n",
      "\n",
      "Epoch 00015: val_loss improved from -0.67997 to -0.68572, saving model to model_keras0516GPU0.h5\n",
      "Epoch 16/200\n",
      "1150/1150 [==============================] - 10s 8ms/step - loss: -0.9306 - val_loss: -0.6974\n",
      "\n",
      "Epoch 00016: val_loss improved from -0.68572 to -0.69741, saving model to model_keras0516GPU0.h5\n",
      "Epoch 17/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9349 - val_loss: -0.7058\n",
      "\n",
      "Epoch 00017: val_loss improved from -0.69741 to -0.70581, saving model to model_keras0516GPU0.h5\n",
      "Epoch 18/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9392 - val_loss: -0.6769\n",
      "\n",
      "Epoch 00018: val_loss did not improve from -0.70581\n",
      "Epoch 19/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9405 - val_loss: -0.7085\n",
      "\n",
      "Epoch 00019: val_loss improved from -0.70581 to -0.70852, saving model to model_keras0516GPU0.h5\n",
      "Epoch 20/200\n",
      "1150/1150 [==============================] - 10s 8ms/step - loss: -0.9437 - val_loss: -0.6959\n",
      "\n",
      "Epoch 00020: val_loss did not improve from -0.70852\n",
      "Epoch 21/200\n",
      "1150/1150 [==============================] - 10s 8ms/step - loss: -0.9471 - val_loss: -0.7056\n",
      "\n",
      "Epoch 00021: val_loss did not improve from -0.70852\n",
      "Epoch 22/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9479 - val_loss: -0.7577\n",
      "\n",
      "Epoch 00022: val_loss improved from -0.70852 to -0.75772, saving model to model_keras0516GPU0.h5\n",
      "Epoch 23/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9502 - val_loss: -0.7038\n",
      "\n",
      "Epoch 00023: val_loss did not improve from -0.75772\n",
      "Epoch 24/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9522 - val_loss: -0.6987\n",
      "\n",
      "Epoch 00024: val_loss did not improve from -0.75772\n",
      "Epoch 25/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9544 - val_loss: -0.7494\n",
      "\n",
      "Epoch 00025: val_loss did not improve from -0.75772\n",
      "Epoch 26/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9572 - val_loss: -0.7659\n",
      "\n",
      "Epoch 00026: val_loss improved from -0.75772 to -0.76592, saving model to model_keras0516GPU0.h5\n",
      "Epoch 27/200\n",
      "1150/1150 [==============================] - 10s 8ms/step - loss: -0.9587 - val_loss: -0.7051\n",
      "\n",
      "Epoch 00027: val_loss did not improve from -0.76592\n",
      "Epoch 28/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9606 - val_loss: -0.6967\n",
      "\n",
      "Epoch 00028: val_loss did not improve from -0.76592\n",
      "Epoch 29/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9616 - val_loss: -0.7120\n",
      "\n",
      "Epoch 00029: val_loss did not improve from -0.76592\n",
      "Epoch 30/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9631 - val_loss: -0.6956\n",
      "\n",
      "Epoch 00030: val_loss did not improve from -0.76592\n",
      "Epoch 31/200\n",
      "1150/1150 [==============================] - 10s 8ms/step - loss: -0.9636 - val_loss: -0.6812\n",
      "\n",
      "Epoch 00031: val_loss did not improve from -0.76592\n",
      "Epoch 32/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9651 - val_loss: -0.7037\n",
      "\n",
      "Epoch 00032: val_loss did not improve from -0.76592\n",
      "Epoch 33/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9650 - val_loss: -0.7140\n",
      "\n",
      "Epoch 00033: val_loss did not improve from -0.76592\n",
      "Epoch 34/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9644 - val_loss: -0.7238\n",
      "\n",
      "Epoch 00034: val_loss did not improve from -0.76592\n",
      "Epoch 35/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9662 - val_loss: -0.6999\n",
      "\n",
      "Epoch 00035: val_loss did not improve from -0.76592\n",
      "Epoch 36/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9673 - val_loss: -0.7135\n",
      "\n",
      "Epoch 00036: val_loss did not improve from -0.76592\n",
      "Epoch 37/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9672 - val_loss: -0.7049\n",
      "\n",
      "Epoch 00037: val_loss did not improve from -0.76592\n",
      "Epoch 38/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9686 - val_loss: -0.7156\n",
      "\n",
      "Epoch 00038: val_loss did not improve from -0.76592\n",
      "Epoch 39/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9690 - val_loss: -0.6743\n",
      "\n",
      "Epoch 00039: val_loss did not improve from -0.76592\n",
      "Epoch 40/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9693 - val_loss: -0.6890\n",
      "\n",
      "Epoch 00040: val_loss did not improve from -0.76592\n",
      "Epoch 41/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9704 - val_loss: -0.7233\n",
      "\n",
      "Epoch 00041: val_loss did not improve from -0.76592\n",
      "Epoch 42/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9699 - val_loss: -0.6818\n",
      "\n",
      "Epoch 00042: val_loss did not improve from -0.76592\n",
      "Epoch 43/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9701 - val_loss: -0.7142\n",
      "\n",
      "Epoch 00043: val_loss did not improve from -0.76592\n",
      "Epoch 44/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9710 - val_loss: -0.7229\n",
      "\n",
      "Epoch 00044: val_loss did not improve from -0.76592\n",
      "Epoch 45/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9720 - val_loss: -0.7252\n",
      "\n",
      "Epoch 00045: val_loss did not improve from -0.76592\n",
      "Epoch 46/200\n",
      "1150/1150 [==============================] - 9s 8ms/step - loss: -0.9721 - val_loss: -0.7157\n",
      "\n",
      "Epoch 00046: val_loss did not improve from -0.76592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feffeef7470>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CheckPoint = keras.callbacks.ModelCheckpoint(\"Model_keras.h5\", monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "autoencoder.fit([train_img[:1150],train_flo[:1150],train_rflo[:1150]], train_seg[:1150],\n",
    "        shuffle=True,\n",
    "        epochs=200,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=([train_img[1150:],train_flo[1150:],train_rflo[1150:]],train_seg[1150:]),callbacks=[EarlyStopping,checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conv to last lyr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols = 256, 256\n",
    "img_channels = 1\n",
    "\n",
    "batch_size =50\n",
    "latent_dim = 256\n",
    "nb_epoch = 50\n",
    "intermediate_dim =256\n",
    "original_dim = img_rows*img_cols\n",
    "LRelu = 'relu'\n",
    "\n",
    "#USE = 'autoencoder'\n",
    "#USE = 'vae'\n",
    "#encoder:\n",
    "\n",
    "input_img = Input(shape=(img_rows, img_cols,img_channels))\n",
    "input_flo = Input(shape=(img_rows, img_cols,3))\n",
    "input_rflo = Input(shape=(img_rows, img_cols,3))\n",
    "\n",
    "concate_flo = Concatenate()([input_flo,input_rflo])\n",
    "conv_flo_1 = Conv2D(20, (3, 3), padding='same',kernel_initializer='normal',dilation_rate=2)(concate_flo)\n",
    "conv_flo_1 = Activation('relu')(conv_flo_1)\n",
    "conv_flo_1 = BatchNormalization()(conv_flo_1)\n",
    "#maxpool_flo_1 = MaxPooling2D((2, 2),padding='same')(conv_flo_1)\n",
    "conv_flo_2 = Conv2D(20, (3, 3), padding='same',kernel_initializer='normal',dilation_rate=2)(conv_flo_1)\n",
    "conv_flo_2 = Activation('relu')(conv_flo_2)\n",
    "conv_flo_2 = BatchNormalization()(conv_flo_2)\n",
    "#maxpool_flo_2 = MaxPooling2D((2, 2),  padding='same')(conv_flo_2)\n",
    "conv_flo_3 = Conv2D(20, (3, 3),padding='same',kernel_initializer='normal',dilation_rate=1)(conv_flo_2)\n",
    "conv_flo_3 = Activation(LRelu)(conv_flo_3)\n",
    "conv_flo_3 = BatchNormalization()(conv_flo_3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "conv_1 = Conv2D(20, (3, 3), padding='same',kernel_initializer='normal',dilation_rate=2)(input_img)\n",
    "conv_1 = Activation('relu')(conv_1)\n",
    "conv_1 = BatchNormalization()(conv_1)\n",
    "maxpool_1 = MaxPooling2D((2, 2),padding='same')(conv_1)\n",
    "#maxpool_1 = Dropout(0.3)(maxpool_1)\n",
    "\n",
    "conv_2 = Conv2D(20, (3, 3), padding='same',kernel_initializer='normal',dilation_rate=2)(maxpool_1)\n",
    "conv_2 = Activation('relu')(conv_2)\n",
    "#conv_2 = Dropout(0.3)(conv_2)\n",
    "conv_2 = BatchNormalization()(conv_2)\n",
    "maxpool_2 = MaxPooling2D((2, 2),  padding='same')(conv_2)\n",
    "#maxpool_2 = Dropout(0.3)(maxpool_2)\n",
    "\n",
    "conv_3 = Conv2D(20, (3, 3),padding='same',kernel_initializer='normal',dilation_rate=1)(maxpool_2)\n",
    "conv_3 = Activation(LRelu)(conv_3)\n",
    "#conv_3 = Dropout(0.3)(conv_3)\n",
    "conv_3 = BatchNormalization()(conv_3)\n",
    "maxpool_3 = MaxPooling2D((2, 2),  padding='same')(conv_3)\n",
    "#maxpool_3 = Dropout(0.3)(maxpool_3)\n",
    "\n",
    "conv_4 = Conv2D(20, (3, 3),padding='same',kernel_initializer='normal',dilation_rate=1)(maxpool_3)\n",
    "conv_4 = Activation(LRelu)(conv_4)\n",
    "#conv_4 = Dropout(0.3)(conv_4)\n",
    "conv_4 = BatchNormalization()(conv_4)\n",
    "maxpool_4 = MaxPooling2D((2, 2),  padding='same')(conv_4)\n",
    "#maxpool_4 = Dropout(0.3)(maxpool_4)\n",
    "\n",
    "#conv_5 = Conv2D(20, (3, 3), activation='relu', padding='same',kernel_initializer='normal')(maxpool_4)\n",
    "#maxpool_5 = MaxPooling2D((2, 2),  padding='same')(conv_5)\n",
    "\n",
    "\n",
    "#x = Conv2D(5, (3, 3), activation='relu', padding='same',kernel_initializer='normal')(x)\n",
    "#x = MaxPooling2D((2, 2),  padding='same')(x)\n",
    "\n",
    "\n",
    "visual = Flatten()(maxpool_4)\n",
    "h_1 = Dense(intermediate_dim, activation='relu')(visual)#relu?\n",
    "encoded = Dense(latent_dim, activation='tanh')(h_1)# relu?\n",
    "\n",
    "\n",
    "h_3 = Dense(intermediate_dim,activation=LRelu)(encoded)#for AE\n",
    "\n",
    "h_4 = Dense(20*16*16,activation=LRelu)(h_3)\n",
    "h_5 = Reshape((16,16,20))(h_4)\n",
    "\n",
    "\n",
    "#conv_6 = Conv2D(20, (3, 3), activation='relu', padding='same',kernel_initializer='normal')(h_5)\n",
    "#upsample_6 = UpSampling2D((2, 2))(conv_6)\n",
    "\n",
    "conv_7 = Conv2D(20, (3, 3), activation='relu', padding='same',kernel_initializer='normal')(h_5)\n",
    "upsample_7 = UpSampling2D((2, 2))(conv_7)\n",
    "\n",
    "upsample_7 = Concatenate()([upsample_7,conv_4])\n",
    "conv_8 = Conv2D(20, (3, 3), activation='relu', padding='same',kernel_initializer='normal')(upsample_7)\n",
    "upsample_8 = UpSampling2D((2, 2))(conv_8)\n",
    "\n",
    "upsample_8 = Concatenate()([upsample_8,conv_3])\n",
    "conv_9 = Conv2D(10, (3, 3), activation='relu', padding='same',kernel_initializer='normal')(upsample_8)\n",
    "upsample_9 = UpSampling2D((2, 2))(conv_9)\n",
    "\n",
    "upsample_9 = Concatenate()([upsample_9,conv_2])\n",
    "conv_10 = Conv2D(5,  (3, 3), activation='relu',padding='same',kernel_initializer='normal')(upsample_9)\n",
    "upsample_10 = UpSampling2D((2, 2))(conv_10)\n",
    "\n",
    "upsample_10 = Concatenate()([upsample_10,conv_1,conv_flo_3])\n",
    "decoded = Conv2D(2, (3, 3), activation='softmax', padding='same')(upsample_10)\n",
    "#decoded = Flatten()(decoded)\n",
    "#decoded = Dense(256*256,activation='softmax')(decoded)\n",
    "\n",
    "\n",
    "EarlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=0, mode='auto')\n",
    "checkpoint = ModelCheckpoint('model_keras0516GPU0.h5',monitor ='val_loss',verbose = 1,save_best_only = True)\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true[:,:,:,1]) \n",
    "    y_pred_f = K.flatten(y_pred[:,:,:,1]) \n",
    "    intersection = K.sum(y_true_f * y_pred_f) \n",
    "    return (2. * intersection + 1e-6) / (K.sum(K.square(y_true_f)) + K.sum(K.square(y_pred_f)) + 1e-6)\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "#def ae_loss(x, decoded):  \n",
    "#    xent_loss = original_dim * objectives.mean_squared_error(x,decoded)\n",
    "#    return xent_loss\n",
    "adam = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "autoencoder = Model(inputs=[input_img,input_flo,input_rflo], outputs=decoded)\n",
    "autoencoder.compile(optimizer=adam, loss=dice_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1150 samples, validate on 127 samples\n",
      "Epoch 1/200\n",
      "1150/1150 [==============================] - 17s 15ms/step - loss: -0.2013 - val_loss: -0.0755\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.07548, saving model to model_keras0516GPU0.h5\n",
      "Epoch 2/200\n",
      "1150/1150 [==============================] - 13s 11ms/step - loss: -0.2368 - val_loss: -0.1041\n",
      "\n",
      "Epoch 00002: val_loss improved from -0.07548 to -0.10408, saving model to model_keras0516GPU0.h5\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to lock file, errno = 11, error message = 'Resource temporarily unavailable')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-1195240a27a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         validation_data=([train_img[1150:],train_flo[1150:],train_rflo[1150:]],train_seg[1150:]),callbacks=[EarlyStopping,checkpoint])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    215\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    442\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to lock file, errno = 11, error message = 'Resource temporarily unavailable')"
     ]
    }
   ],
   "source": [
    "#CheckPoint = keras.callbacks.ModelCheckpoint(\"Model_keras.h5\", monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "autoencoder.fit([train_img[:1150],train_flo[:1150],train_rflo[:1150]], train_seg[:1150],\n",
    "        shuffle=True,\n",
    "        epochs=200,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=([train_img[1150:],train_flo[1150:],train_rflo[1150:]],train_seg[1150:]),callbacks=[EarlyStopping,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

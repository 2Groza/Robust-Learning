{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsun/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras import optimizers\n",
    "import keras.layers.advanced_activations\n",
    "import scipy\n",
    "import random\n",
    "import matplotlib.pyplot as plt  \n",
    "from scipy.stats import norm  \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import VarianceScaling,RandomNormal\n",
    "from keras.layers import Input, Dense, Dropout, Conv2D, MaxPooling2D, concatenate\n",
    "from keras.layers import UpSampling2D, Reshape, Lambda, Flatten, Activation,Concatenate,Add\n",
    "from keras.models import Model  \n",
    "from keras.optimizers import SGD, Adadelta, Adagrad,Adam\n",
    "from keras import backend as K  \n",
    "from keras import objectives  \n",
    "from keras.utils.vis_utils import plot_model  \n",
    "from keras.utils import np_utils, generic_utils\n",
    "import sys \n",
    "import tensorflow as tf\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'\n",
    "import sklearn\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import NullFormatter\n",
    "%matplotlib inline\n",
    "from sklearn import manifold, datasets,cluster\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = np.load(\"img_set_log.npy\").reshape([-1,256,256,1])\n",
    "train_seg = np.load(\"seg_set_log.npy\").reshape([-1,256,256,1])\n",
    "train_flo = np.load(\"flo_set_rgb_log.npy\")\n",
    "train_rflo = np.load(\"rflo_set_rgb_log.npy\")\n",
    "#train_img = np.log(train_img+1.0)\n",
    "train_img = train_img/train_img.max()\n",
    "train_seg = np_utils.to_categorical(train_seg, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsun/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:57: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"co...)`\n"
     ]
    }
   ],
   "source": [
    "img_rows, img_cols = 256, 256\n",
    "img_channels = 1\n",
    "\n",
    "batch_size =50\n",
    "latent_dim = 256\n",
    "nb_epoch = 50\n",
    "intermediate_dim =256\n",
    "original_dim = img_rows*img_cols\n",
    "LRelu = 'relu'\n",
    "\n",
    "#USE = 'autoencoder'\n",
    "#USE = 'vae'\n",
    "#encoder:\n",
    "input_img = Input(shape=(img_rows, img_cols,img_channels))\n",
    "input_flo = Input(shape=(img_rows, img_cols,3))\n",
    "input_rflo = Input(shape=(img_rows, img_cols,3))\n",
    "conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(input_img)\n",
    "conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "drop4 = Dropout(0.5)(conv4)\n",
    "pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "merge6 = Concatenate()([drop4,up6])\n",
    "conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "merge7 = Concatenate()([conv3,up7])\n",
    "conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "merge8 =Concatenate()([conv2,up8])\n",
    "conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "merge9 = Concatenate()([conv1,up9,input_flo,input_rflo])\n",
    "conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "conv10 = Conv2D(2, 1, activation = 'softmax')(conv9)\n",
    "\n",
    "unet = Model(input = [input_img,input_flo,input_rflo], output = conv10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#decoded = Conv2D(2, (3, 3), activation='softmax', padding='same')(upsample_10)\n",
    "#decoded = Flatten()(decoded)\n",
    "#decoded = Dense(256*256,activation='softmax')(decoded)\n",
    "\n",
    "\n",
    "EarlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=7, verbose=0, mode='auto')\n",
    "checkpoint = ModelCheckpoint('model_keras_unet.h5',monitor ='val_loss',verbose = 1,save_best_only = True)\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true[:,:,:,1]) \n",
    "    y_pred_f = K.flatten(y_pred[:,:,:,1]) \n",
    "    intersection = K.sum(y_true_f * y_pred_f) \n",
    "    return (2. * intersection + 1e-6) / (K.sum(K.square(y_true_f)) + K.sum(K.square(y_pred_f)) + 1e-6)\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "#def ae_loss(x, decoded):  \n",
    "#    xent_loss = original_dim * objectives.mean_squared_error(x,decoded)\n",
    "#    return xent_loss\n",
    "adam = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "#autoencoder = Model(inputs=input_img, outputs=decoded)\n",
    "unet.compile(optimizer=adam, loss=dice_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1150 samples, validate on 127 samples\n",
      "Epoch 1/200\n",
      "1150/1150 [==============================] - 81s 70ms/step - loss: -0.2940 - val_loss: -0.2080\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.20804, saving model to model_keras_unet.h5\n",
      "Epoch 2/200\n",
      "1150/1150 [==============================] - 73s 63ms/step - loss: -0.4243 - val_loss: -0.2540\n",
      "\n",
      "Epoch 00002: val_loss improved from -0.20804 to -0.25405, saving model to model_keras_unet.h5\n",
      "Epoch 3/200\n",
      "1150/1150 [==============================] - 73s 63ms/step - loss: -0.4367 - val_loss: -0.0400\n",
      "\n",
      "Epoch 00003: val_loss did not improve from -0.25405\n",
      "Epoch 4/200\n",
      "1150/1150 [==============================] - 73s 63ms/step - loss: -0.4358 - val_loss: -0.2582\n",
      "\n",
      "Epoch 00004: val_loss improved from -0.25405 to -0.25819, saving model to model_keras_unet.h5\n",
      "Epoch 5/200\n",
      "1150/1150 [==============================] - 72s 63ms/step - loss: -0.4504 - val_loss: -0.3522\n",
      "\n",
      "Epoch 00005: val_loss improved from -0.25819 to -0.35216, saving model to model_keras_unet.h5\n",
      "Epoch 6/200\n",
      "1150/1150 [==============================] - 72s 63ms/step - loss: -0.4590 - val_loss: -0.2165\n",
      "\n",
      "Epoch 00006: val_loss did not improve from -0.35216\n",
      "Epoch 7/200\n",
      "1150/1150 [==============================] - 72s 63ms/step - loss: -0.4828 - val_loss: -0.4308\n",
      "\n",
      "Epoch 00007: val_loss improved from -0.35216 to -0.43082, saving model to model_keras_unet.h5\n",
      "Epoch 8/200\n",
      "1150/1150 [==============================] - 72s 63ms/step - loss: -0.4955 - val_loss: -0.4118\n",
      "\n",
      "Epoch 00008: val_loss did not improve from -0.43082\n",
      "Epoch 9/200\n",
      "1150/1150 [==============================] - 72s 63ms/step - loss: -0.5123 - val_loss: -0.4109\n",
      "\n",
      "Epoch 00009: val_loss did not improve from -0.43082\n",
      "Epoch 10/200\n",
      "1150/1150 [==============================] - 72s 63ms/step - loss: -0.5219 - val_loss: -0.3718\n",
      "\n",
      "Epoch 00010: val_loss did not improve from -0.43082\n",
      "Epoch 11/200\n",
      "1150/1150 [==============================] - 72s 63ms/step - loss: -0.5494 - val_loss: -0.5054\n",
      "\n",
      "Epoch 00011: val_loss improved from -0.43082 to -0.50543, saving model to model_keras_unet.h5\n",
      "Epoch 12/200\n",
      "1150/1150 [==============================] - 72s 63ms/step - loss: -0.5531 - val_loss: -0.5529\n",
      "\n",
      "Epoch 00012: val_loss improved from -0.50543 to -0.55295, saving model to model_keras_unet.h5\n",
      "Epoch 13/200\n",
      "1150/1150 [==============================] - 72s 63ms/step - loss: -0.5705 - val_loss: -0.5515\n",
      "\n",
      "Epoch 00013: val_loss did not improve from -0.55295\n",
      "Epoch 14/200\n",
      "1150/1150 [==============================] - 72s 63ms/step - loss: -0.5729 - val_loss: -0.4920\n",
      "\n",
      "Epoch 00014: val_loss did not improve from -0.55295\n",
      "Epoch 15/200\n",
      "1150/1150 [==============================] - 72s 62ms/step - loss: -0.6151 - val_loss: -0.5892\n",
      "\n",
      "Epoch 00015: val_loss improved from -0.55295 to -0.58924, saving model to model_keras_unet.h5\n",
      "Epoch 16/200\n",
      "1150/1150 [==============================] - 72s 62ms/step - loss: -0.6410 - val_loss: -0.6118\n",
      "\n",
      "Epoch 00016: val_loss improved from -0.58924 to -0.61176, saving model to model_keras_unet.h5\n",
      "Epoch 17/200\n",
      "1150/1150 [==============================] - 72s 63ms/step - loss: -0.6471 - val_loss: -0.5484\n",
      "\n",
      "Epoch 00017: val_loss did not improve from -0.61176\n",
      "Epoch 18/200\n",
      "1150/1150 [==============================] - 72s 62ms/step - loss: -0.6615 - val_loss: -0.6381\n",
      "\n",
      "Epoch 00018: val_loss improved from -0.61176 to -0.63811, saving model to model_keras_unet.h5\n",
      "Epoch 19/200\n",
      "1150/1150 [==============================] - 72s 62ms/step - loss: -0.6892 - val_loss: -0.5498\n",
      "\n",
      "Epoch 00019: val_loss did not improve from -0.63811\n",
      "Epoch 20/200\n",
      "1150/1150 [==============================] - 72s 63ms/step - loss: -0.6893 - val_loss: -0.6371\n",
      "\n",
      "Epoch 00020: val_loss did not improve from -0.63811\n",
      "Epoch 21/200\n",
      "1150/1150 [==============================] - 71s 62ms/step - loss: -0.6949 - val_loss: -0.5895\n",
      "\n",
      "Epoch 00021: val_loss did not improve from -0.63811\n",
      "Epoch 22/200\n",
      "1150/1150 [==============================] - 72s 62ms/step - loss: -0.6987 - val_loss: -0.6781\n",
      "\n",
      "Epoch 00022: val_loss improved from -0.63811 to -0.67809, saving model to model_keras_unet.h5\n",
      "Epoch 23/200\n",
      "1150/1150 [==============================] - 72s 62ms/step - loss: -0.7260 - val_loss: -0.6756\n",
      "\n",
      "Epoch 00023: val_loss did not improve from -0.67809\n",
      "Epoch 24/200\n",
      "1150/1150 [==============================] - 72s 62ms/step - loss: -0.7286 - val_loss: -0.6865\n",
      "\n",
      "Epoch 00024: val_loss improved from -0.67809 to -0.68651, saving model to model_keras_unet.h5\n",
      "Epoch 25/200\n",
      "1150/1150 [==============================] - 71s 62ms/step - loss: -0.7283 - val_loss: -0.7084\n",
      "\n",
      "Epoch 00025: val_loss improved from -0.68651 to -0.70836, saving model to model_keras_unet.h5\n",
      "Epoch 26/200\n",
      "1150/1150 [==============================] - 71s 62ms/step - loss: -0.7531 - val_loss: -0.7153\n",
      "\n",
      "Epoch 00026: val_loss improved from -0.70836 to -0.71531, saving model to model_keras_unet.h5\n",
      "Epoch 27/200\n",
      "1150/1150 [==============================] - 72s 62ms/step - loss: -0.7668 - val_loss: -0.7047\n",
      "\n",
      "Epoch 00027: val_loss did not improve from -0.71531\n",
      "Epoch 28/200\n",
      "1150/1150 [==============================] - 71s 62ms/step - loss: -0.7599 - val_loss: -0.7116\n",
      "\n",
      "Epoch 00028: val_loss did not improve from -0.71531\n",
      "Epoch 29/200\n",
      "1150/1150 [==============================] - 72s 62ms/step - loss: -0.7875 - val_loss: -0.7149\n",
      "\n",
      "Epoch 00029: val_loss did not improve from -0.71531\n",
      "Epoch 30/200\n",
      "1150/1150 [==============================] - 72s 62ms/step - loss: -0.7964 - val_loss: -0.7302\n",
      "\n",
      "Epoch 00030: val_loss improved from -0.71531 to -0.73017, saving model to model_keras_unet.h5\n",
      "Epoch 31/200\n",
      "1150/1150 [==============================] - 71s 62ms/step - loss: -0.8036 - val_loss: -0.7286\n",
      "\n",
      "Epoch 00031: val_loss did not improve from -0.73017\n",
      "Epoch 32/200\n",
      "1150/1150 [==============================] - 71s 62ms/step - loss: -0.8033 - val_loss: -0.7381\n",
      "\n",
      "Epoch 00032: val_loss improved from -0.73017 to -0.73808, saving model to model_keras_unet.h5\n",
      "Epoch 33/200\n",
      "1150/1150 [==============================] - 71s 62ms/step - loss: -0.8163 - val_loss: -0.7293\n",
      "\n",
      "Epoch 00033: val_loss did not improve from -0.73808\n",
      "Epoch 34/200\n",
      "1150/1150 [==============================] - 71s 62ms/step - loss: -0.8199 - val_loss: -0.7000\n",
      "\n",
      "Epoch 00034: val_loss did not improve from -0.73808\n",
      "Epoch 35/200\n",
      "1150/1150 [==============================] - 71s 62ms/step - loss: -0.8250 - val_loss: -0.7176\n",
      "\n",
      "Epoch 00035: val_loss did not improve from -0.73808\n",
      "Epoch 36/200\n",
      "1150/1150 [==============================] - 72s 62ms/step - loss: -0.8215 - val_loss: -0.7369\n",
      "\n",
      "Epoch 00036: val_loss did not improve from -0.73808\n",
      "Epoch 37/200\n",
      "1150/1150 [==============================] - 71s 62ms/step - loss: -0.8243 - val_loss: -0.7370\n",
      "\n",
      "Epoch 00037: val_loss did not improve from -0.73808\n",
      "Epoch 38/200\n",
      "1150/1150 [==============================] - 72s 62ms/step - loss: -0.8325 - val_loss: -0.7437\n",
      "\n",
      "Epoch 00038: val_loss improved from -0.73808 to -0.74369, saving model to model_keras_unet.h5\n",
      "Epoch 39/200\n",
      "1150/1150 [==============================] - 72s 62ms/step - loss: -0.8445 - val_loss: -0.7264\n",
      "\n",
      "Epoch 00039: val_loss did not improve from -0.74369\n",
      "Epoch 40/200\n",
      "1150/1150 [==============================] - 72s 62ms/step - loss: -0.8496 - val_loss: -0.7421\n",
      "\n",
      "Epoch 00040: val_loss did not improve from -0.74369\n",
      "Epoch 41/200\n",
      "1150/1150 [==============================] - 72s 62ms/step - loss: -0.8550 - val_loss: -0.7211\n",
      "\n",
      "Epoch 00041: val_loss did not improve from -0.74369\n",
      "Epoch 42/200\n",
      "1150/1150 [==============================] - 71s 62ms/step - loss: -0.8561 - val_loss: -0.7360\n",
      "\n",
      "Epoch 00042: val_loss did not improve from -0.74369\n",
      "Epoch 43/200\n",
      "1150/1150 [==============================] - 71s 62ms/step - loss: -0.8887 - val_loss: -0.7570\n",
      "\n",
      "Epoch 00043: val_loss improved from -0.74369 to -0.75696, saving model to model_keras_unet.h5\n",
      "Epoch 44/200\n",
      "1150/1150 [==============================] - 71s 62ms/step - loss: -0.8652 - val_loss: -0.7789\n",
      "\n",
      "Epoch 00044: val_loss improved from -0.75696 to -0.77892, saving model to model_keras_unet.h5\n",
      "Epoch 45/200\n",
      "1150/1150 [==============================] - 71s 62ms/step - loss: -0.9145 - val_loss: -0.8218\n",
      "\n",
      "Epoch 00045: val_loss improved from -0.77892 to -0.82177, saving model to model_keras_unet.h5\n",
      "Epoch 46/200\n",
      "1150/1150 [==============================] - 71s 62ms/step - loss: -0.9300 - val_loss: -0.8232\n",
      "\n",
      "Epoch 00046: val_loss improved from -0.82177 to -0.82317, saving model to model_keras_unet.h5\n",
      "Epoch 47/200\n",
      "1150/1150 [==============================] - 71s 62ms/step - loss: -0.9524 - val_loss: -0.8073\n",
      "\n",
      "Epoch 00047: val_loss did not improve from -0.82317\n",
      "Epoch 48/200\n",
      "1150/1150 [==============================] - 72s 62ms/step - loss: -0.9580 - val_loss: -0.7920\n",
      "\n",
      "Epoch 00048: val_loss did not improve from -0.82317\n",
      "Epoch 49/200\n",
      "1150/1150 [==============================] - 72s 62ms/step - loss: -0.9596 - val_loss: -0.8046\n",
      "\n",
      "Epoch 00049: val_loss did not improve from -0.82317\n",
      "Epoch 50/200\n",
      "1150/1150 [==============================] - 71s 62ms/step - loss: -0.9572 - val_loss: -0.8053\n",
      "\n",
      "Epoch 00050: val_loss did not improve from -0.82317\n",
      "Epoch 51/200\n",
      "1150/1150 [==============================] - 72s 62ms/step - loss: -0.9623 - val_loss: -0.8111\n",
      "\n",
      "Epoch 00051: val_loss did not improve from -0.82317\n",
      "Epoch 52/200\n",
      "1150/1150 [==============================] - 71s 62ms/step - loss: -0.9643 - val_loss: -0.8041\n",
      "\n",
      "Epoch 00052: val_loss did not improve from -0.82317\n",
      "Epoch 53/200\n",
      "1150/1150 [==============================] - 71s 62ms/step - loss: -0.9657 - val_loss: -0.7983\n",
      "\n",
      "Epoch 00053: val_loss did not improve from -0.82317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8600099e48>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet.fit([train_img[:1150],train_flo[:1150],train_rflo[:1150]], train_seg[:1150],\n",
    "        shuffle=True,\n",
    "        epochs=200,\n",
    "        batch_size=20,\n",
    "        validation_data=([train_img[1150:],train_flo[1150:],train_rflo[1150:]],train_seg[1150:]),callbacks=[EarlyStopping,checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsun/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:79: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"co...)`\n"
     ]
    }
   ],
   "source": [
    "img_rows, img_cols = 256, 256\n",
    "img_channels = 1\n",
    "\n",
    "batch_size =50\n",
    "latent_dim = 256\n",
    "nb_epoch = 50\n",
    "intermediate_dim =256\n",
    "original_dim = img_rows*img_cols\n",
    "LRelu = 'relu'\n",
    "\n",
    "#USE = 'autoencoder'\n",
    "#USE = 'vae'\n",
    "#encoder:\n",
    "input_img = Input(shape=(img_rows, img_cols,img_channels))\n",
    "input_flo = Input(shape=(img_rows, img_cols,3))\n",
    "input_rflo = Input(shape=(img_rows, img_cols,3))\n",
    "\n",
    "concate_flo = Concatenate()([input_flo,input_rflo])\n",
    "conv_flo_1 = Conv2D(20, (3, 3), padding='same',kernel_initializer='normal',dilation_rate=2)(concate_flo)\n",
    "conv_flo_1 = Activation('relu')(conv_flo_1)\n",
    "conv_flo_1 = BatchNormalization()(conv_flo_1)\n",
    "maxpool_flo_1 = MaxPooling2D((2, 2),padding='same')(conv_flo_1)\n",
    "conv_flo_2 = Conv2D(20, (3, 3), padding='same',kernel_initializer='normal',dilation_rate=2)(maxpool_flo_1)\n",
    "conv_flo_2 = Activation('relu')(conv_flo_2)\n",
    "conv_flo_2 = BatchNormalization()(conv_flo_2)\n",
    "maxpool_flo_2 = MaxPooling2D((2, 2),  padding='same')(conv_flo_2)\n",
    "conv_flo_3 = Conv2D(20, (3, 3),padding='same',kernel_initializer='normal',dilation_rate=1)(maxpool_flo_2)\n",
    "conv_flo_3 = Activation(LRelu)(conv_flo_3)\n",
    "conv_flo_3 = BatchNormalization()(conv_flo_3)\n",
    "maxpool_flo_3 = MaxPooling2D((2, 2),  padding='same')(conv_flo_3)\n",
    "conv_flo_4 = Conv2D(20, (3, 3),padding='same',kernel_initializer='normal',dilation_rate=1)(maxpool_flo_3)\n",
    "conv_flo_4 = Activation(LRelu)(conv_flo_4)\n",
    "conv_flo_4 = BatchNormalization()(conv_flo_4)\n",
    "maxpool_flo_4 = MaxPooling2D((2, 2),  padding='same')(conv_flo_4)\n",
    "\n",
    "\n",
    "conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(input_img)\n",
    "conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "drop4 = Dropout(0.5)(conv4)\n",
    "pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "drop5 = Concatenate()([maxpool_flo_4,drop5])\n",
    "\n",
    "up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "merge6 = Concatenate()([drop4,up6])\n",
    "conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "merge7 = Concatenate()([conv3,up7])\n",
    "conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "merge8 =Concatenate()([conv2,up8])\n",
    "conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "merge9 = Concatenate()([conv1,up9])\n",
    "conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "conv10 = Conv2D(2, 1, activation = 'softmax')(conv9)\n",
    "\n",
    "unet = Model(input = [input_img,input_flo,input_rflo], output = conv10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#decoded = Conv2D(2, (3, 3), activation='softmax', padding='same')(upsample_10)\n",
    "#decoded = Flatten()(decoded)\n",
    "#decoded = Dense(256*256,activation='softmax')(decoded)\n",
    "\n",
    "\n",
    "EarlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=7, verbose=0, mode='auto')\n",
    "checkpoint = ModelCheckpoint('model_keras_unet_mid.h5',monitor ='val_loss',verbose = 1,save_best_only = True)\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true[:,:,:,1]) \n",
    "    y_pred_f = K.flatten(y_pred[:,:,:,1]) \n",
    "    intersection = K.sum(y_true_f * y_pred_f) \n",
    "    return (2. * intersection + 1e-6) / (K.sum(K.square(y_true_f)) + K.sum(K.square(y_pred_f)) + 1e-6)\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "#def ae_loss(x, decoded):  \n",
    "#    xent_loss = original_dim * objectives.mean_squared_error(x,decoded)\n",
    "#    return xent_loss\n",
    "adam = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "#autoencoder = Model(inputs=input_img, outputs=decoded)\n",
    "unet.compile(optimizer=adam, loss=dice_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1150 samples, validate on 127 samples\n",
      "Epoch 1/200\n",
      "1150/1150 [==============================] - 89s 77ms/step - loss: -0.3256 - val_loss: -0.1619\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.16193, saving model to model_keras_unet_mid.h5\n",
      "Epoch 2/200\n",
      "1150/1150 [==============================] - 74s 64ms/step - loss: -0.4288 - val_loss: -0.3183\n",
      "\n",
      "Epoch 00002: val_loss improved from -0.16193 to -0.31830, saving model to model_keras_unet_mid.h5\n",
      "Epoch 3/200\n",
      "1150/1150 [==============================] - 74s 64ms/step - loss: -0.4757 - val_loss: -0.3378\n",
      "\n",
      "Epoch 00003: val_loss improved from -0.31830 to -0.33775, saving model to model_keras_unet_mid.h5\n",
      "Epoch 4/200\n",
      "1150/1150 [==============================] - 74s 64ms/step - loss: -0.5107 - val_loss: -0.3259\n",
      "\n",
      "Epoch 00004: val_loss did not improve from -0.33775\n",
      "Epoch 5/200\n",
      "1150/1150 [==============================] - 74s 64ms/step - loss: -0.5255 - val_loss: -0.4304\n",
      "\n",
      "Epoch 00005: val_loss improved from -0.33775 to -0.43038, saving model to model_keras_unet_mid.h5\n",
      "Epoch 6/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.5511 - val_loss: -0.4512\n",
      "\n",
      "Epoch 00006: val_loss improved from -0.43038 to -0.45118, saving model to model_keras_unet_mid.h5\n",
      "Epoch 7/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.5616 - val_loss: -0.5302\n",
      "\n",
      "Epoch 00007: val_loss improved from -0.45118 to -0.53019, saving model to model_keras_unet_mid.h5\n",
      "Epoch 8/200\n",
      "1150/1150 [==============================] - 73s 63ms/step - loss: -0.5809 - val_loss: -0.5143\n",
      "\n",
      "Epoch 00008: val_loss did not improve from -0.53019\n",
      "Epoch 9/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.5987 - val_loss: -0.4939\n",
      "\n",
      "Epoch 00009: val_loss did not improve from -0.53019\n",
      "Epoch 10/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.6077 - val_loss: -0.4500\n",
      "\n",
      "Epoch 00010: val_loss did not improve from -0.53019\n",
      "Epoch 11/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.6254 - val_loss: -0.5287\n",
      "\n",
      "Epoch 00011: val_loss did not improve from -0.53019\n",
      "Epoch 12/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.6379 - val_loss: -0.5758\n",
      "\n",
      "Epoch 00012: val_loss improved from -0.53019 to -0.57577, saving model to model_keras_unet_mid.h5\n",
      "Epoch 13/200\n",
      "1150/1150 [==============================] - 74s 64ms/step - loss: -0.6601 - val_loss: -0.6044\n",
      "\n",
      "Epoch 00013: val_loss improved from -0.57577 to -0.60441, saving model to model_keras_unet_mid.h5\n",
      "Epoch 14/200\n",
      "1150/1150 [==============================] - 74s 64ms/step - loss: -0.6698 - val_loss: -0.6490\n",
      "\n",
      "Epoch 00014: val_loss improved from -0.60441 to -0.64898, saving model to model_keras_unet_mid.h5\n",
      "Epoch 15/200\n",
      "1150/1150 [==============================] - 74s 64ms/step - loss: -0.6780 - val_loss: -0.5595\n",
      "\n",
      "Epoch 00015: val_loss did not improve from -0.64898\n",
      "Epoch 16/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.6973 - val_loss: -0.5837\n",
      "\n",
      "Epoch 00016: val_loss did not improve from -0.64898\n",
      "Epoch 17/200\n",
      "1150/1150 [==============================] - 74s 64ms/step - loss: -0.7172 - val_loss: -0.6373\n",
      "\n",
      "Epoch 00017: val_loss did not improve from -0.64898\n",
      "Epoch 18/200\n",
      "1150/1150 [==============================] - 74s 64ms/step - loss: -0.7216 - val_loss: -0.5462\n",
      "\n",
      "Epoch 00018: val_loss did not improve from -0.64898\n",
      "Epoch 19/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.7217 - val_loss: -0.6654\n",
      "\n",
      "Epoch 00019: val_loss improved from -0.64898 to -0.66542, saving model to model_keras_unet_mid.h5\n",
      "Epoch 20/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.7575 - val_loss: -0.6912\n",
      "\n",
      "Epoch 00020: val_loss improved from -0.66542 to -0.69119, saving model to model_keras_unet_mid.h5\n",
      "Epoch 21/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.7533 - val_loss: -0.6877\n",
      "\n",
      "Epoch 00021: val_loss did not improve from -0.69119\n",
      "Epoch 22/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.7715 - val_loss: -0.7149\n",
      "\n",
      "Epoch 00022: val_loss improved from -0.69119 to -0.71495, saving model to model_keras_unet_mid.h5\n",
      "Epoch 23/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.7710 - val_loss: -0.6817\n",
      "\n",
      "Epoch 00023: val_loss did not improve from -0.71495\n",
      "Epoch 24/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.7845 - val_loss: -0.7101\n",
      "\n",
      "Epoch 00024: val_loss did not improve from -0.71495\n",
      "Epoch 25/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.7963 - val_loss: -0.7206\n",
      "\n",
      "Epoch 00025: val_loss improved from -0.71495 to -0.72056, saving model to model_keras_unet_mid.h5\n",
      "Epoch 26/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.8006 - val_loss: -0.7186\n",
      "\n",
      "Epoch 00026: val_loss did not improve from -0.72056\n",
      "Epoch 27/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.8031 - val_loss: -0.7229\n",
      "\n",
      "Epoch 00027: val_loss improved from -0.72056 to -0.72291, saving model to model_keras_unet_mid.h5\n",
      "Epoch 28/200\n",
      "1150/1150 [==============================] - 74s 64ms/step - loss: -0.8134 - val_loss: -0.7342\n",
      "\n",
      "Epoch 00028: val_loss improved from -0.72291 to -0.73422, saving model to model_keras_unet_mid.h5\n",
      "Epoch 29/200\n",
      "1150/1150 [==============================] - 74s 64ms/step - loss: -0.8224 - val_loss: -0.7323\n",
      "\n",
      "Epoch 00029: val_loss did not improve from -0.73422\n",
      "Epoch 30/200\n",
      "1150/1150 [==============================] - 74s 64ms/step - loss: -0.8149 - val_loss: -0.7400\n",
      "\n",
      "Epoch 00030: val_loss improved from -0.73422 to -0.74002, saving model to model_keras_unet_mid.h5\n",
      "Epoch 31/200\n",
      "1150/1150 [==============================] - 74s 64ms/step - loss: -0.8252 - val_loss: -0.7339\n",
      "\n",
      "Epoch 00031: val_loss did not improve from -0.74002\n",
      "Epoch 32/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.8199 - val_loss: -0.7296\n",
      "\n",
      "Epoch 00032: val_loss did not improve from -0.74002\n",
      "Epoch 33/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.8351 - val_loss: -0.7369\n",
      "\n",
      "Epoch 00033: val_loss did not improve from -0.74002\n",
      "Epoch 34/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.8376 - val_loss: -0.7452\n",
      "\n",
      "Epoch 00034: val_loss improved from -0.74002 to -0.74521, saving model to model_keras_unet_mid.h5\n",
      "Epoch 35/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.8385 - val_loss: -0.7471\n",
      "\n",
      "Epoch 00035: val_loss improved from -0.74521 to -0.74708, saving model to model_keras_unet_mid.h5\n",
      "Epoch 36/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.8492 - val_loss: -0.7560\n",
      "\n",
      "Epoch 00036: val_loss improved from -0.74708 to -0.75601, saving model to model_keras_unet_mid.h5\n",
      "Epoch 37/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.8518 - val_loss: -0.7464\n",
      "\n",
      "Epoch 00037: val_loss did not improve from -0.75601\n",
      "Epoch 38/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.8521 - val_loss: -0.7373\n",
      "\n",
      "Epoch 00038: val_loss did not improve from -0.75601\n",
      "Epoch 39/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.8565 - val_loss: -0.7521\n",
      "\n",
      "Epoch 00039: val_loss did not improve from -0.75601\n",
      "Epoch 40/200\n",
      "1150/1150 [==============================] - 74s 64ms/step - loss: -0.8612 - val_loss: -0.7569\n",
      "\n",
      "Epoch 00040: val_loss improved from -0.75601 to -0.75691, saving model to model_keras_unet_mid.h5\n",
      "Epoch 41/200\n",
      "1150/1150 [==============================] - 74s 64ms/step - loss: -0.8657 - val_loss: -0.7622\n",
      "\n",
      "Epoch 00041: val_loss improved from -0.75691 to -0.76222, saving model to model_keras_unet_mid.h5\n",
      "Epoch 42/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.8667 - val_loss: -0.7635\n",
      "\n",
      "Epoch 00042: val_loss improved from -0.76222 to -0.76349, saving model to model_keras_unet_mid.h5\n",
      "Epoch 43/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.8707 - val_loss: -0.7565\n",
      "\n",
      "Epoch 00043: val_loss did not improve from -0.76349\n",
      "Epoch 44/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.8728 - val_loss: -0.7583\n",
      "\n",
      "Epoch 00044: val_loss did not improve from -0.76349\n",
      "Epoch 45/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.8714 - val_loss: -0.7628\n",
      "\n",
      "Epoch 00045: val_loss did not improve from -0.76349\n",
      "Epoch 46/200\n",
      "1150/1150 [==============================] - 74s 64ms/step - loss: -0.8723 - val_loss: -0.7469\n",
      "\n",
      "Epoch 00046: val_loss did not improve from -0.76349\n",
      "Epoch 47/200\n",
      "1150/1150 [==============================] - 74s 64ms/step - loss: -0.8761 - val_loss: -0.7645\n",
      "\n",
      "Epoch 00047: val_loss improved from -0.76349 to -0.76452, saving model to model_keras_unet_mid.h5\n",
      "Epoch 48/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.8801 - val_loss: -0.7661\n",
      "\n",
      "Epoch 00048: val_loss improved from -0.76452 to -0.76608, saving model to model_keras_unet_mid.h5\n",
      "Epoch 49/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.8825 - val_loss: -0.7757\n",
      "\n",
      "Epoch 00049: val_loss improved from -0.76608 to -0.77572, saving model to model_keras_unet_mid.h5\n",
      "Epoch 50/200\n",
      "1150/1150 [==============================] - 74s 64ms/step - loss: -0.8849 - val_loss: -0.7694\n",
      "\n",
      "Epoch 00050: val_loss did not improve from -0.77572\n",
      "Epoch 51/200\n",
      "1150/1150 [==============================] - 74s 64ms/step - loss: -0.8857 - val_loss: -0.7643\n",
      "\n",
      "Epoch 00051: val_loss did not improve from -0.77572\n",
      "Epoch 52/200\n",
      "1150/1150 [==============================] - 74s 64ms/step - loss: -0.8872 - val_loss: -0.7726\n",
      "\n",
      "Epoch 00052: val_loss did not improve from -0.77572\n",
      "Epoch 53/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.8908 - val_loss: -0.7713\n",
      "\n",
      "Epoch 00053: val_loss did not improve from -0.77572\n",
      "Epoch 54/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.8913 - val_loss: -0.7737\n",
      "\n",
      "Epoch 00054: val_loss did not improve from -0.77572\n",
      "Epoch 55/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.8924 - val_loss: -0.7659\n",
      "\n",
      "Epoch 00055: val_loss did not improve from -0.77572\n",
      "Epoch 56/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.8954 - val_loss: -0.7729\n",
      "\n",
      "Epoch 00056: val_loss did not improve from -0.77572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa5fc860ac8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet.fit([train_img[:1150],train_flo[:1150],train_rflo[:1150]], train_seg[:1150],\n",
    "        shuffle=True,\n",
    "        epochs=200,\n",
    "        batch_size=20,\n",
    "        validation_data=([train_img[1150:],train_flo[1150:],train_rflo[1150:]],train_seg[1150:]),callbacks=[EarlyStopping,checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conv last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsun/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:75: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"co...)`\n"
     ]
    }
   ],
   "source": [
    "img_rows, img_cols = 256, 256\n",
    "img_channels = 1\n",
    "\n",
    "batch_size =50\n",
    "latent_dim = 256\n",
    "nb_epoch = 50\n",
    "intermediate_dim =256\n",
    "original_dim = img_rows*img_cols\n",
    "LRelu = 'relu'\n",
    "\n",
    "#USE = 'autoencoder'\n",
    "#USE = 'vae'\n",
    "#encoder:\n",
    "input_img = Input(shape=(img_rows, img_cols,img_channels))\n",
    "input_flo = Input(shape=(img_rows, img_cols,3))\n",
    "input_rflo = Input(shape=(img_rows, img_cols,3))\n",
    "\n",
    "concate_flo = Concatenate()([input_flo,input_rflo])\n",
    "conv_flo_1 = Conv2D(20, (3, 3), padding='same',kernel_initializer='normal',dilation_rate=2)(concate_flo)\n",
    "conv_flo_1 = Activation('relu')(conv_flo_1)\n",
    "conv_flo_1 = BatchNormalization()(conv_flo_1)\n",
    "#maxpool_flo_1 = MaxPooling2D((2, 2),padding='same')(conv_flo_1)\n",
    "conv_flo_2 = Conv2D(20, (3, 3), padding='same',kernel_initializer='normal',dilation_rate=2)(conv_flo_1)\n",
    "conv_flo_2 = Activation('relu')(conv_flo_2)\n",
    "conv_flo_2 = BatchNormalization()(conv_flo_2)\n",
    "#maxpool_flo_2 = MaxPooling2D((2, 2),  padding='same')(conv_flo_2)\n",
    "conv_flo_3 = Conv2D(20, (3, 3),padding='same',kernel_initializer='normal',dilation_rate=1)(conv_flo_2)\n",
    "conv_flo_3 = Activation(LRelu)(conv_flo_3)\n",
    "conv_flo_3 = BatchNormalization()(conv_flo_3)\n",
    "\n",
    "\n",
    "\n",
    "conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(input_img)\n",
    "conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "drop4 = Dropout(0.5)(conv4)\n",
    "pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "#drop5 = Concatenate()([maxpool_flo_4,drop5])\n",
    "\n",
    "up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "merge6 = Concatenate()([drop4,up6])\n",
    "conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "merge7 = Concatenate()([conv3,up7])\n",
    "conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "merge8 =Concatenate()([conv2,up8])\n",
    "conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "merge9 = Concatenate()([conv1,up9,conv_flo_3])\n",
    "conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "conv10 = Conv2D(2, 1, activation = 'softmax')(conv9)\n",
    "\n",
    "unet = Model(input = [input_img,input_flo,input_rflo], output = conv10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#decoded = Conv2D(2, (3, 3), activation='softmax', padding='same')(upsample_10)\n",
    "#decoded = Flatten()(decoded)\n",
    "#decoded = Dense(256*256,activation='softmax')(decoded)\n",
    "\n",
    "\n",
    "EarlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=7, verbose=0, mode='auto')\n",
    "checkpoint = ModelCheckpoint('model_keras_unet.h5',monitor ='val_loss',verbose = 1,save_best_only = True)\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true[:,:,:,1]) \n",
    "    y_pred_f = K.flatten(y_pred[:,:,:,1]) \n",
    "    intersection = K.sum(y_true_f * y_pred_f) \n",
    "    return (2. * intersection + 1e-6) / (K.sum(K.square(y_true_f)) + K.sum(K.square(y_pred_f)) + 1e-6)\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "#def ae_loss(x, decoded):  \n",
    "#    xent_loss = original_dim * objectives.mean_squared_error(x,decoded)\n",
    "#    return xent_loss\n",
    "adam = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "#autoencoder = Model(inputs=input_img, outputs=decoded)\n",
    "unet.compile(optimizer=adam, loss=dice_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1150 samples, validate on 127 samples\n",
      "Epoch 1/200\n",
      "1150/1150 [==============================] - 76s 66ms/step - loss: -0.9381 - val_loss: -0.8167\n",
      "\n",
      "Epoch 00001: val_loss did not improve from -0.82090\n",
      "Epoch 2/200\n",
      "1150/1150 [==============================] - 79s 68ms/step - loss: -0.9238 - val_loss: -0.8172\n",
      "\n",
      "Epoch 00002: val_loss did not improve from -0.82090\n",
      "Epoch 3/200\n",
      "1150/1150 [==============================] - 79s 68ms/step - loss: -0.9410 - val_loss: -0.8058\n",
      "\n",
      "Epoch 00003: val_loss did not improve from -0.82090\n",
      "Epoch 4/200\n",
      "1150/1150 [==============================] - 79s 68ms/step - loss: -0.9469 - val_loss: -0.8269\n",
      "\n",
      "Epoch 00004: val_loss improved from -0.82090 to -0.82685, saving model to model_keras_unet.h5\n",
      "Epoch 5/200\n",
      "1150/1150 [==============================] - 78s 68ms/step - loss: -0.9525 - val_loss: -0.8122\n",
      "\n",
      "Epoch 00005: val_loss did not improve from -0.82685\n",
      "Epoch 6/200\n",
      "1150/1150 [==============================] - 78s 68ms/step - loss: -0.9556 - val_loss: -0.8247\n",
      "\n",
      "Epoch 00006: val_loss did not improve from -0.82685\n",
      "Epoch 7/200\n",
      "1150/1150 [==============================] - 78s 68ms/step - loss: -0.9581 - val_loss: -0.8286\n",
      "\n",
      "Epoch 00007: val_loss improved from -0.82685 to -0.82862, saving model to model_keras_unet.h5\n",
      "Epoch 8/200\n",
      "1150/1150 [==============================] - 78s 68ms/step - loss: -0.9599 - val_loss: -0.8360\n",
      "\n",
      "Epoch 00008: val_loss improved from -0.82862 to -0.83596, saving model to model_keras_unet.h5\n",
      "Epoch 9/200\n",
      "1150/1150 [==============================] - 78s 68ms/step - loss: -0.9628 - val_loss: -0.8165\n",
      "\n",
      "Epoch 00009: val_loss did not improve from -0.83596\n",
      "Epoch 10/200\n",
      "1150/1150 [==============================] - 78s 68ms/step - loss: -0.9614 - val_loss: -0.8275\n",
      "\n",
      "Epoch 00010: val_loss did not improve from -0.83596\n",
      "Epoch 11/200\n",
      "1150/1150 [==============================] - 78s 68ms/step - loss: -0.9633 - val_loss: -0.8335\n",
      "\n",
      "Epoch 00011: val_loss did not improve from -0.83596\n",
      "Epoch 12/200\n",
      "1150/1150 [==============================] - 78s 68ms/step - loss: -0.9627 - val_loss: -0.8289\n",
      "\n",
      "Epoch 00012: val_loss did not improve from -0.83596\n",
      "Epoch 13/200\n",
      "1150/1150 [==============================] - 78s 68ms/step - loss: -0.9643 - val_loss: -0.8191\n",
      "\n",
      "Epoch 00013: val_loss did not improve from -0.83596\n",
      "Epoch 14/200\n",
      "1150/1150 [==============================] - 78s 68ms/step - loss: -0.9675 - val_loss: -0.8275\n",
      "\n",
      "Epoch 00014: val_loss did not improve from -0.83596\n",
      "Epoch 15/200\n",
      "1150/1150 [==============================] - 78s 68ms/step - loss: -0.9664 - val_loss: -0.8261\n",
      "\n",
      "Epoch 00015: val_loss did not improve from -0.83596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faa6fe787b8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet.fit([train_img[:1150],train_flo[:1150],train_rflo[:1150]], train_seg[:1150],\n",
    "        shuffle=True,\n",
    "        epochs=200,\n",
    "        batch_size=20,\n",
    "        validation_data=([train_img[1150:],train_flo[1150:],train_rflo[1150:]],train_seg[1150:]),callbacks=[EarlyStopping,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8425"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 跑一个不用flo的baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsun/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:56: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"
     ]
    }
   ],
   "source": [
    "img_rows, img_cols = 256, 256\n",
    "img_channels = 1\n",
    "\n",
    "batch_size =50\n",
    "latent_dim = 256\n",
    "nb_epoch = 50\n",
    "intermediate_dim =256\n",
    "original_dim = img_rows*img_cols\n",
    "LRelu = 'relu'\n",
    "\n",
    "#USE = 'autoencoder'\n",
    "#USE = 'vae'\n",
    "#encoder:\n",
    "input_img = Input(shape=(img_rows, img_cols,img_channels))\n",
    "\n",
    "conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(input_img)\n",
    "conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "drop4 = Dropout(0.5)(conv4)\n",
    "pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "merge6 = Concatenate()([drop4,up6])\n",
    "conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "merge7 = Concatenate()([conv3,up7])\n",
    "conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "merge8 =Concatenate()([conv2,up8])\n",
    "conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "merge9 = Concatenate()([conv1,up9])\n",
    "conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "conv10 = Conv2D(2, 1, activation = 'softmax')(conv9)\n",
    "\n",
    "unet = Model(input = input_img, output = conv10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#decoded = Conv2D(2, (3, 3), activation='softmax', padding='same')(upsample_10)\n",
    "#decoded = Flatten()(decoded)\n",
    "#decoded = Dense(256*256,activation='softmax')(decoded)\n",
    "\n",
    "\n",
    "EarlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=7, verbose=0, mode='auto')\n",
    "checkpoint = ModelCheckpoint('model_keras_unet.h5',monitor ='val_loss',verbose = 1,save_best_only = True)\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true[:,:,:,1]) \n",
    "    y_pred_f = K.flatten(y_pred[:,:,:,1]) \n",
    "    intersection = K.sum(y_true_f * y_pred_f) \n",
    "    return (2. * intersection + 1e-6) / (K.sum(K.square(y_true_f)) + K.sum(K.square(y_pred_f)) + 1e-6)\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "#def ae_loss(x, decoded):  \n",
    "#    xent_loss = original_dim * objectives.mean_squared_error(x,decoded)\n",
    "#    return xent_loss\n",
    "adam = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "#autoencoder = Model(inputs=input_img, outputs=decoded)\n",
    "unet.compile(optimizer=adam, loss=dice_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1150 samples, validate on 127 samples\n",
      "Epoch 1/200\n",
      "1150/1150 [==============================] - 73s 64ms/step - loss: -0.3209 - val_loss: -0.0034\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.00336, saving model to model_keras_unet.h5\n",
      "Epoch 2/200\n",
      "1150/1150 [==============================] - 71s 61ms/step - loss: -0.4366 - val_loss: -0.3590\n",
      "\n",
      "Epoch 00002: val_loss improved from -0.00336 to -0.35899, saving model to model_keras_unet.h5\n",
      "Epoch 3/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.4729 - val_loss: -0.3981\n",
      "\n",
      "Epoch 00003: val_loss improved from -0.35899 to -0.39809, saving model to model_keras_unet.h5\n",
      "Epoch 4/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.4725 - val_loss: -0.3908\n",
      "\n",
      "Epoch 00004: val_loss did not improve from -0.39809\n",
      "Epoch 5/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.5092 - val_loss: -0.3845\n",
      "\n",
      "Epoch 00005: val_loss did not improve from -0.39809\n",
      "Epoch 6/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.5194 - val_loss: -0.5132\n",
      "\n",
      "Epoch 00006: val_loss improved from -0.39809 to -0.51325, saving model to model_keras_unet.h5\n",
      "Epoch 7/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.5256 - val_loss: -0.4661\n",
      "\n",
      "Epoch 00007: val_loss did not improve from -0.51325\n",
      "Epoch 8/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.5568 - val_loss: -0.4872\n",
      "\n",
      "Epoch 00008: val_loss did not improve from -0.51325\n",
      "Epoch 9/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.5640 - val_loss: -0.3710\n",
      "\n",
      "Epoch 00009: val_loss did not improve from -0.51325\n",
      "Epoch 10/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.5786 - val_loss: -0.5358\n",
      "\n",
      "Epoch 00010: val_loss improved from -0.51325 to -0.53584, saving model to model_keras_unet.h5\n",
      "Epoch 11/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.6115 - val_loss: -0.6464\n",
      "\n",
      "Epoch 00011: val_loss improved from -0.53584 to -0.64643, saving model to model_keras_unet.h5\n",
      "Epoch 12/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.5784 - val_loss: -0.4690\n",
      "\n",
      "Epoch 00012: val_loss did not improve from -0.64643\n",
      "Epoch 13/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.6247 - val_loss: -0.5921\n",
      "\n",
      "Epoch 00013: val_loss did not improve from -0.64643\n",
      "Epoch 14/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.6675 - val_loss: -0.6058\n",
      "\n",
      "Epoch 00014: val_loss did not improve from -0.64643\n",
      "Epoch 15/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.6785 - val_loss: -0.5365\n",
      "\n",
      "Epoch 00015: val_loss did not improve from -0.64643\n",
      "Epoch 16/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.6874 - val_loss: -0.5594\n",
      "\n",
      "Epoch 00016: val_loss did not improve from -0.64643\n",
      "Epoch 17/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.6838 - val_loss: -0.6523\n",
      "\n",
      "Epoch 00017: val_loss improved from -0.64643 to -0.65228, saving model to model_keras_unet.h5\n",
      "Epoch 18/200\n",
      "1150/1150 [==============================] - 70s 60ms/step - loss: -0.7114 - val_loss: -0.6861\n",
      "\n",
      "Epoch 00018: val_loss improved from -0.65228 to -0.68611, saving model to model_keras_unet.h5\n",
      "Epoch 19/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.7314 - val_loss: -0.6812\n",
      "\n",
      "Epoch 00019: val_loss did not improve from -0.68611\n",
      "Epoch 20/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.7271 - val_loss: -0.6643\n",
      "\n",
      "Epoch 00020: val_loss did not improve from -0.68611\n",
      "Epoch 21/200\n",
      "1150/1150 [==============================] - 70s 60ms/step - loss: -0.7445 - val_loss: -0.6922\n",
      "\n",
      "Epoch 00021: val_loss improved from -0.68611 to -0.69216, saving model to model_keras_unet.h5\n",
      "Epoch 22/200\n",
      "1150/1150 [==============================] - 70s 60ms/step - loss: -0.7613 - val_loss: -0.7033\n",
      "\n",
      "Epoch 00022: val_loss improved from -0.69216 to -0.70332, saving model to model_keras_unet.h5\n",
      "Epoch 23/200\n",
      "1150/1150 [==============================] - 69s 60ms/step - loss: -0.7724 - val_loss: -0.7149\n",
      "\n",
      "Epoch 00023: val_loss improved from -0.70332 to -0.71487, saving model to model_keras_unet.h5\n",
      "Epoch 24/200\n",
      "1150/1150 [==============================] - 70s 60ms/step - loss: -0.7812 - val_loss: -0.7095\n",
      "\n",
      "Epoch 00024: val_loss did not improve from -0.71487\n",
      "Epoch 25/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.7703 - val_loss: -0.6930\n",
      "\n",
      "Epoch 00025: val_loss did not improve from -0.71487\n",
      "Epoch 26/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.7875 - val_loss: -0.7092\n",
      "\n",
      "Epoch 00026: val_loss did not improve from -0.71487\n",
      "Epoch 27/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.7957 - val_loss: -0.7055\n",
      "\n",
      "Epoch 00027: val_loss did not improve from -0.71487\n",
      "Epoch 28/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.7998 - val_loss: -0.7154\n",
      "\n",
      "Epoch 00028: val_loss improved from -0.71487 to -0.71537, saving model to model_keras_unet.h5\n",
      "Epoch 29/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.7987 - val_loss: -0.6394\n",
      "\n",
      "Epoch 00029: val_loss did not improve from -0.71537\n",
      "Epoch 30/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.7966 - val_loss: -0.6503\n",
      "\n",
      "Epoch 00030: val_loss did not improve from -0.71537\n",
      "Epoch 31/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.8206 - val_loss: -0.7294\n",
      "\n",
      "Epoch 00031: val_loss improved from -0.71537 to -0.72936, saving model to model_keras_unet.h5\n",
      "Epoch 32/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.8264 - val_loss: -0.7047\n",
      "\n",
      "Epoch 00032: val_loss did not improve from -0.72936\n",
      "Epoch 33/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.8332 - val_loss: -0.7375\n",
      "\n",
      "Epoch 00033: val_loss improved from -0.72936 to -0.73748, saving model to model_keras_unet.h5\n",
      "Epoch 34/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.8381 - val_loss: -0.7329\n",
      "\n",
      "Epoch 00034: val_loss did not improve from -0.73748\n",
      "Epoch 35/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.8429 - val_loss: -0.7369\n",
      "\n",
      "Epoch 00035: val_loss did not improve from -0.73748\n",
      "Epoch 36/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.8307 - val_loss: -0.7376\n",
      "\n",
      "Epoch 00036: val_loss improved from -0.73748 to -0.73757, saving model to model_keras_unet.h5\n",
      "Epoch 37/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.8458 - val_loss: -0.7276\n",
      "\n",
      "Epoch 00037: val_loss did not improve from -0.73757\n",
      "Epoch 38/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.8528 - val_loss: -0.7268\n",
      "\n",
      "Epoch 00038: val_loss did not improve from -0.73757\n",
      "Epoch 39/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.8576 - val_loss: -0.7307\n",
      "\n",
      "Epoch 00039: val_loss did not improve from -0.73757\n",
      "Epoch 40/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.8569 - val_loss: -0.6842\n",
      "\n",
      "Epoch 00040: val_loss did not improve from -0.73757\n",
      "Epoch 41/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.9228 - val_loss: -0.7714\n",
      "\n",
      "Epoch 00041: val_loss improved from -0.73757 to -0.77138, saving model to model_keras_unet.h5\n",
      "Epoch 42/200\n",
      "1150/1150 [==============================] - 69s 60ms/step - loss: -0.9584 - val_loss: -0.7727\n",
      "\n",
      "Epoch 00042: val_loss improved from -0.77138 to -0.77273, saving model to model_keras_unet.h5\n",
      "Epoch 43/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.9617 - val_loss: -0.7940\n",
      "\n",
      "Epoch 00043: val_loss improved from -0.77273 to -0.79404, saving model to model_keras_unet.h5\n",
      "Epoch 44/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.9661 - val_loss: -0.8282\n",
      "\n",
      "Epoch 00044: val_loss improved from -0.79404 to -0.82821, saving model to model_keras_unet.h5\n",
      "Epoch 45/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.9627 - val_loss: -0.8185\n",
      "\n",
      "Epoch 00045: val_loss did not improve from -0.82821\n",
      "Epoch 46/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.9658 - val_loss: -0.8187\n",
      "\n",
      "Epoch 00046: val_loss did not improve from -0.82821\n",
      "Epoch 47/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.9676 - val_loss: -0.8002\n",
      "\n",
      "Epoch 00047: val_loss did not improve from -0.82821\n",
      "Epoch 48/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.9696 - val_loss: -0.8180\n",
      "\n",
      "Epoch 00048: val_loss did not improve from -0.82821\n",
      "Epoch 49/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.9692 - val_loss: -0.8382\n",
      "\n",
      "Epoch 00049: val_loss improved from -0.82821 to -0.83823, saving model to model_keras_unet.h5\n",
      "Epoch 50/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.9722 - val_loss: -0.8327\n",
      "\n",
      "Epoch 00050: val_loss did not improve from -0.83823\n",
      "Epoch 51/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.9735 - val_loss: -0.8117\n",
      "\n",
      "Epoch 00051: val_loss did not improve from -0.83823\n",
      "Epoch 52/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.9747 - val_loss: -0.8246\n",
      "\n",
      "Epoch 00052: val_loss did not improve from -0.83823\n",
      "Epoch 53/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.9753 - val_loss: -0.8227\n",
      "\n",
      "Epoch 00053: val_loss did not improve from -0.83823\n",
      "Epoch 54/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.9754 - val_loss: -0.8224\n",
      "\n",
      "Epoch 00054: val_loss did not improve from -0.83823\n",
      "Epoch 55/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.9765 - val_loss: -0.8296\n",
      "\n",
      "Epoch 00055: val_loss did not improve from -0.83823\n",
      "Epoch 56/200\n",
      "1150/1150 [==============================] - 70s 61ms/step - loss: -0.9754 - val_loss: -0.8142\n",
      "\n",
      "Epoch 00056: val_loss did not improve from -0.83823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f906bb06f60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet.fit(train_img[:1150], train_seg[:1150],\n",
    "        shuffle=True,\n",
    "        epochs=200,\n",
    "        batch_size=20,\n",
    "        validation_data=(train_img[1150:],train_seg[1150:]),callbacks=[EarlyStopping,checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 不用flo 加上Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsun/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:56: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"
     ]
    }
   ],
   "source": [
    "img_rows, img_cols = 256, 256\n",
    "img_channels = 1\n",
    "\n",
    "batch_size =20\n",
    "latent_dim = 256\n",
    "nb_epoch = 50\n",
    "intermediate_dim =256\n",
    "original_dim = img_rows*img_cols\n",
    "LRelu = 'relu'\n",
    "\n",
    "#USE = 'autoencoder'\n",
    "#USE = 'vae'\n",
    "#encoder:\n",
    "input_img = Input(shape=(img_rows, img_cols,img_channels))\n",
    "\n",
    "conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(input_img)\n",
    "conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "drop4 = Dropout(0.5)(conv4)\n",
    "pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "merge6 = Concatenate()([drop4,up6])\n",
    "conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "merge7 = Concatenate()([conv3,up7])\n",
    "conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "merge8 =Concatenate()([conv2,up8])\n",
    "conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "merge9 = Concatenate()([conv1,up9])\n",
    "conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "conv10 = Conv2D(2, 1, activation = 'softmax')(conv9)\n",
    "\n",
    "unet = Model(input = input_img, output = conv10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#decoded = Conv2D(2, (3, 3), activation='softmax', padding='same')(upsample_10)\n",
    "#decoded = Flatten()(decoded)\n",
    "#decoded = Dense(256*256,activation='softmax')(decoded)\n",
    "\n",
    "\n",
    "EarlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=7, verbose=0, mode='auto')\n",
    "checkpoint = ModelCheckpoint('model_keras_unet.h5',monitor ='val_loss',verbose = 1,save_best_only = True)\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true[:,:,:,1]) \n",
    "    y_pred_f = K.flatten(y_pred[:,:,:,1]) \n",
    "    intersection = K.sum(y_true_f * y_pred_f) \n",
    "    return (2. * intersection + 1e-6) / (K.sum(K.square(y_true_f)) + K.sum(K.square(y_pred_f)) + 1e-6)\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "#def ae_loss(x, decoded):  \n",
    "#    xent_loss = original_dim * objectives.mean_squared_error(x,decoded)\n",
    "#    return xent_loss\n",
    "adam = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "#autoencoder = Model(inputs=input_img, outputs=decoded)\n",
    "unet.compile(optimizer=adam, loss=dice_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using data augmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsun/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., steps_per_epoch=57, validation_data=(array([[[..., epochs=200, verbose=1, callbacks=[<keras.ca..., max_queue_size=100)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "57/57 [==============================] - 72s 1s/step - loss: -0.3119 - val_loss: -0.0125\n",
      "Epoch 2/200\n",
      "57/57 [==============================] - 70s 1s/step - loss: -0.4292 - val_loss: -0.2413\n",
      "Epoch 3/200\n",
      "57/57 [==============================] - 69s 1s/step - loss: -0.4387 - val_loss: -0.2535\n",
      "Epoch 4/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.4365 - val_loss: -0.2842\n",
      "Epoch 5/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.4406 - val_loss: -0.2639\n",
      "Epoch 6/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.4408 - val_loss: -0.2436\n",
      "Epoch 7/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.4494 - val_loss: -0.2822\n",
      "Epoch 8/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.4418 - val_loss: -0.3193\n",
      "Epoch 9/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.4516 - val_loss: -0.2180\n",
      "Epoch 10/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.4579 - val_loss: -0.0669\n",
      "Epoch 11/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.4456 - val_loss: -0.3284\n",
      "Epoch 12/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.4487 - val_loss: -0.2840\n",
      "Epoch 13/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.4543 - val_loss: -0.3452\n",
      "Epoch 14/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.4594 - val_loss: -0.3917\n",
      "Epoch 15/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.4680 - val_loss: -0.2778\n",
      "Epoch 16/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.4741 - val_loss: -0.4206\n",
      "Epoch 17/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.4643 - val_loss: -0.4528\n",
      "Epoch 18/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.4763 - val_loss: -0.4055\n",
      "Epoch 19/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.4811 - val_loss: -0.4805\n",
      "Epoch 20/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.4810 - val_loss: -0.4379\n",
      "Epoch 21/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.4847 - val_loss: -0.4882\n",
      "Epoch 22/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.4992 - val_loss: -0.3679\n",
      "Epoch 23/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.4966 - val_loss: -0.3732\n",
      "Epoch 24/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.4988 - val_loss: -0.5067\n",
      "Epoch 25/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.5085 - val_loss: -0.4133\n",
      "Epoch 26/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.5364 - val_loss: -0.4142\n",
      "Epoch 27/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.5229 - val_loss: -0.4956\n",
      "Epoch 28/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.5372 - val_loss: -0.4639\n",
      "Epoch 29/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.5494 - val_loss: -0.5013\n",
      "Epoch 30/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.5932 - val_loss: -0.5363\n",
      "Epoch 31/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.6171 - val_loss: -0.5273\n",
      "Epoch 32/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.6063 - val_loss: -0.5300\n",
      "Epoch 33/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.6302 - val_loss: -0.5708\n",
      "Epoch 34/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.6338 - val_loss: -0.5387\n",
      "Epoch 35/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.6369 - val_loss: -0.5848\n",
      "Epoch 36/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.6473 - val_loss: -0.6104\n",
      "Epoch 37/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.6459 - val_loss: -0.6129\n",
      "Epoch 38/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.6546 - val_loss: -0.6184\n",
      "Epoch 39/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.6646 - val_loss: -0.5576\n",
      "Epoch 40/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.6546 - val_loss: -0.6105\n",
      "Epoch 41/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.6684 - val_loss: -0.6014\n",
      "Epoch 42/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.6790 - val_loss: -0.6367\n",
      "Epoch 43/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.6879 - val_loss: -0.6259\n",
      "Epoch 44/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.6876 - val_loss: -0.6168\n",
      "Epoch 45/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.6867 - val_loss: -0.6272\n",
      "Epoch 46/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.6919 - val_loss: -0.6348\n",
      "Epoch 47/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.6916 - val_loss: -0.6309\n",
      "Epoch 48/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.7029 - val_loss: -0.6381\n",
      "Epoch 49/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.7133 - val_loss: -0.6220\n",
      "Epoch 50/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.7082 - val_loss: -0.6305\n",
      "Epoch 51/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.7136 - val_loss: -0.6322\n",
      "Epoch 52/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.7146 - val_loss: -0.6398\n",
      "Epoch 53/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.7164 - val_loss: -0.6398\n",
      "Epoch 54/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.7176 - val_loss: -0.6371\n",
      "Epoch 55/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.7155 - val_loss: -0.6455\n",
      "Epoch 56/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.7191 - val_loss: -0.6312\n",
      "Epoch 57/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.7204 - val_loss: -0.6177\n",
      "Epoch 58/200\n",
      "57/57 [==============================] - 67s 1s/step - loss: -0.7222 - val_loss: -0.6374\n",
      "Epoch 59/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.7235 - val_loss: -0.6333\n",
      "Epoch 60/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.7219 - val_loss: -0.6278\n",
      "Epoch 61/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.7278 - val_loss: -0.6403\n",
      "Epoch 62/200\n",
      "57/57 [==============================] - 68s 1s/step - loss: -0.7250 - val_loss: -0.6355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fee45101128>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
    "early_stopper = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=7, verbose=0, mode='auto')\n",
    "csv_logger = CSVLogger('Unet_logdata_loss.csv')\n",
    "print(\"using data augmentation\")\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "datagen.fit(train_img[:1150])\n",
    "\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "unet.fit_generator(datagen.flow(train_img[:1150], train_seg[:1150], batch_size=batch_size),\n",
    "                    steps_per_epoch=train_img[:1150].shape[0] // batch_size,\n",
    "                    validation_data=(train_img[1150:],train_seg[1150:]),\n",
    "                    epochs=200, verbose=1, max_q_size=100,\n",
    "                    callbacks=[lr_reducer, early_stopper, csv_logger])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

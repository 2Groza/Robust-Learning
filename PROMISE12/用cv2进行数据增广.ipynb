{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras import optimizers\n",
    "import keras.layers.advanced_activations\n",
    "import scipy\n",
    "import random\n",
    "import matplotlib.pyplot as plt  \n",
    "from scipy.stats import norm  \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import VarianceScaling,RandomNormal\n",
    "from keras.layers import Input, Dense, Dropout, Conv2D, MaxPooling2D, concatenate\n",
    "from keras.layers import UpSampling2D, Reshape, Lambda, Flatten, Activation,Concatenate,Add\n",
    "from keras.models import Model  \n",
    "from keras.optimizers import SGD, Adadelta, Adagrad,Adam\n",
    "from keras import backend as K  \n",
    "from keras import objectives  \n",
    "from keras.utils.vis_utils import plot_model  \n",
    "from keras.utils import np_utils, generic_utils\n",
    "import sys \n",
    "import tensorflow as tf\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import sklearn\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import NullFormatter\n",
    "%matplotlib inline\n",
    "from sklearn import manifold, datasets,cluster\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = np.load(\"img_set_log.npy\").reshape([-1,256,256,1])\n",
    "train_seg = np.load(\"seg_set_log.npy\").reshape([-1,256,256,1])\n",
    "train_flo = np.load(\"flo_set_rgb_log.npy\")\n",
    "train_rflo = np.load(\"rflo_set_rgb_log.npy\")\n",
    "#train_img = np.log(train_img+1.0)\n",
    "train_img = train_img/train_img.max()\n",
    "#train_seg = np_utils.to_categorical(train_seg, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(image, x, y):\n",
    "    # 定义平移矩阵\n",
    "    M = np.float32([[1, 0, x], [0, 1, y]])\n",
    "    shifted = cv2.warpAffine(image, M, (image.shape[1], image.shape[0]))\n",
    "\n",
    "    # 返回转换后的图像\n",
    "    return shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = train_img[1150:]\n",
    "test_seg = train_seg[1150:]\n",
    "test_flo = train_flo[1150:]\n",
    "test_rflo = train_rflo[1150:]\n",
    "train_img = train_img[:1150]\n",
    "train_seg = train_seg[:1150]\n",
    "train_flo = train_flo[:1150]\n",
    "train_rflo = train_rflo[:1150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对train进行flip, shift变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_flip = []\n",
    "train_seg_flip = []\n",
    "train_flo_flip = []\n",
    "train_rflo_flip = []\n",
    "for i in range(1150):\n",
    "    train_img_flip.append(cv2.flip(train_img[i],1))\n",
    "    train_seg_flip.append(cv2.flip(train_seg[i],1))\n",
    "    train_flo_flip.append(cv2.flip(train_flo[i],1))\n",
    "    train_rflo_flip.append(cv2.flip(train_rflo[i],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1150, 256, 256)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_img_flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = np.vstack([train_img[:,:,:,0],train_img_flip])\n",
    "train_seg = np.vstack([train_seg[:,:,:,0],train_seg_flip])\n",
    "train_flo = np.vstack([train_flo,train_flo_flip])\n",
    "train_rflo = np.vstack([train_rflo,train_rflo_flip])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2300, 256, 256)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2300, 256, 256)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_shift = []\n",
    "train_seg_shift = []\n",
    "train_flo_shift = []\n",
    "train_rflo_shift = []\n",
    "for i in range(2300):\n",
    "    #向上2\n",
    "    train_img_shift.append(translate(train_img[i],0,-2))\n",
    "    train_seg_shift.append(np.round(translate(train_seg[i]+0.00001,0,-2)))\n",
    "    train_flo_shift.append(translate(train_flo[i],0,-2))\n",
    "    train_rflo_shift.append(translate(train_rflo[i],0,-2))\n",
    "    \n",
    "    train_img_shift.append(translate(train_img[i],2,0))\n",
    "    train_seg_shift.append(np.round(translate(train_seg[i]+0.00001,2,0)))\n",
    "    train_flo_shift.append(translate(train_flo[i],2,0))\n",
    "    train_rflo_shift.append(translate(train_rflo[i],2,0))\n",
    "    \n",
    "    train_img_shift.append(translate(train_img[i],0,2))\n",
    "    train_seg_shift.append(np.round(translate(train_seg[i]+0.00001,0,2)))\n",
    "    train_flo_shift.append(translate(train_flo[i],0,2))\n",
    "    train_rflo_shift.append(translate(train_rflo[i],0,2))  \n",
    "    \n",
    "    train_img_shift.append(translate(train_img[i],1,1))\n",
    "    train_seg_shift.append(np.round(translate(train_seg[i]+0.00001,1,1)))\n",
    "    train_flo_shift.append(translate(train_flo[i],1,1))\n",
    "    train_rflo_shift.append(translate(train_rflo[i],1,1))    \n",
    "    \n",
    "    train_img_shift.append(translate(train_img[i],1,-1))\n",
    "    train_seg_shift.append(np.round(translate(train_seg[i]+0.00001,1,-1)))\n",
    "    train_flo_shift.append(translate(train_flo[i],1,-1))\n",
    "    train_rflo_shift.append(translate(train_rflo[i],1,-1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11500, 256, 256)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_img_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = np.vstack([train_img,train_img_shift])\n",
    "train_seg = np.vstack([train_seg,train_seg_shift])\n",
    "train_flo = np.vstack([train_flo,train_flo_shift])\n",
    "train_rflo = np.vstack([train_rflo,train_rflo_shift])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13800, 256, 256)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"train_img_aug.npy\",train_img)\n",
    "np.save(\"train_seg_aug.npy\",train_seg)\n",
    "np.save(\"train_flo_aug.npy\",train_flo)\n",
    "np.save(\"train_rflo_aug.npy\",train_rflo)\n",
    "np.save(\"test_img.npy\",test_img)\n",
    "np.save(\"test_seg.npy\",test_seg)\n",
    "np.save(\"test_flo.npy\",test_flo)\n",
    "np.save(\"test_rflo.npy\",test_rflo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127, 256, 256, 3)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(test_flo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

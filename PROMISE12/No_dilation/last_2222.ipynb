{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsun/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras import optimizers\n",
    "import keras.layers.advanced_activations\n",
    "import scipy\n",
    "import random\n",
    "import matplotlib.pyplot as plt  \n",
    "from scipy.stats import norm  \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import VarianceScaling,RandomNormal\n",
    "from keras.layers import Input, Dense, Dropout, Conv2D, MaxPooling2D, concatenate\n",
    "from keras.layers import UpSampling2D, Reshape, Lambda, Flatten, Activation,Concatenate,Add\n",
    "from keras.models import Model  \n",
    "from keras.optimizers import SGD, Adadelta, Adagrad,Adam\n",
    "from keras import backend as K  \n",
    "from keras import objectives  \n",
    "from keras.utils.vis_utils import plot_model  \n",
    "from keras.utils import np_utils, generic_utils\n",
    "import sys \n",
    "import tensorflow as tf\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "import sklearn\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import NullFormatter\n",
    "%matplotlib inline\n",
    "from sklearn import manifold, datasets,cluster\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = np.load(\"../train_img_aug.npy\").reshape([-1,256,256,1])\n",
    "train_seg = np.load(\"../train_seg_aug.npy\").reshape([-1,256,256,1])\n",
    "train_flo = np.load(\"../train_flo_aug.npy\")\n",
    "train_rflo = np.load(\"../train_rflo_aug.npy\")\n",
    "#train_img = np.log(train_img+1.0)\n",
    "train_seg = np_utils.to_categorical(train_seg, 2)\n",
    "\n",
    "test_img = np.load(\"../test_img.npy\").reshape([-1,256,256,1])\n",
    "test_seg = np.load(\"../test_seg.npy\").reshape([-1,256,256,1])\n",
    "test_flo = np.load(\"../test_flo.npy\")\n",
    "test_rflo = np.load(\"../test_rflo.npy\")\n",
    "test_seg = np_utils.to_categorical(test_seg, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsun/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:57: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"co...)`\n"
     ]
    }
   ],
   "source": [
    "img_rows, img_cols = 256, 256\n",
    "img_channels = 1\n",
    "\n",
    "batch_size =50\n",
    "latent_dim = 256\n",
    "nb_epoch = 50\n",
    "intermediate_dim =256\n",
    "original_dim = img_rows*img_cols\n",
    "LRelu = 'relu'\n",
    "\n",
    "#USE = 'autoencoder'\n",
    "#USE = 'vae'\n",
    "#encoder:\n",
    "input_img = Input(shape=(img_rows, img_cols,img_channels))\n",
    "input_flo = Input(shape=(img_rows, img_cols,3))\n",
    "input_rflo = Input(shape=(img_rows, img_cols,3))\n",
    "conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',dilation_rate = 2)(input_img)\n",
    "conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',dilation_rate = 2)(conv1)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',dilation_rate = 2)(pool1)\n",
    "conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',dilation_rate = 2)(conv2)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "drop4 = Dropout(0.5)(conv4)\n",
    "pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "merge6 = Concatenate()([drop4,up6])\n",
    "conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "merge7 = Concatenate()([conv3,up7])\n",
    "conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "merge8 =Concatenate()([conv2,up8])\n",
    "conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "merge9 = Concatenate()([conv1,up9,input_flo,input_rflo])\n",
    "conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "conv10 = Conv2D(2, 1, activation = 'softmax')(conv9)\n",
    "\n",
    "unet = Model(input = [input_img,input_flo,input_rflo], output = conv10)\n",
    "\n",
    "#lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
    "#early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=7, verbose=0, mode='auto')\n",
    "csv_logger = CSVLogger('last2222.csv')\n",
    "\n",
    "#decoded = Conv2D(2, (3, 3), activation='softmax', padding='same')(upsample_10)\n",
    "#decoded = Flatten()(decoded)\n",
    "#decoded = Dense(256*256,activation='softmax')(decoded)\n",
    "\n",
    "\n",
    "EarlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')\n",
    "checkpoint = ModelCheckpoint('last2222.h5',monitor ='val_loss',verbose = 1,save_best_only = True)\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true[:,:,:,1]) \n",
    "    y_pred_f = K.flatten(y_pred[:,:,:,1]) \n",
    "    intersection = K.sum(y_true_f * y_pred_f) \n",
    "    return (2. * intersection + 1e-6) / (K.sum(K.square(y_true_f)) + K.sum(K.square(y_pred_f)) + 1e-6)\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "#def ae_loss(x, decoded):  \n",
    "#    xent_loss = original_dim * objectives.mean_squared_error(x,decoded)\n",
    "#    return xent_loss\n",
    "adam = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "#autoencoder = Model(inputs=input_img, outputs=decoded)\n",
    "unet.compile(optimizer=adam, loss=dice_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13800 samples, validate on 127 samples\n",
      "Epoch 1/200\n",
      "13800/13800 [==============================] - 1006s 73ms/step - loss: -0.5370 - val_loss: -0.5154\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.51538, saving model to last2222.h5\n",
      "Epoch 2/200\n",
      "13800/13800 [==============================] - 990s 72ms/step - loss: -0.7133 - val_loss: -0.7161\n",
      "\n",
      "Epoch 00002: val_loss improved from -0.51538 to -0.71608, saving model to last2222.h5\n",
      "Epoch 3/200\n",
      "13800/13800 [==============================] - 991s 72ms/step - loss: -0.8085 - val_loss: -0.7428\n",
      "\n",
      "Epoch 00003: val_loss improved from -0.71608 to -0.74280, saving model to last2222.h5\n",
      "Epoch 4/200\n",
      "13800/13800 [==============================] - 990s 72ms/step - loss: -0.8483 - val_loss: -0.7209\n",
      "\n",
      "Epoch 00004: val_loss did not improve from -0.74280\n",
      "Epoch 5/200\n",
      "13800/13800 [==============================] - 989s 72ms/step - loss: -0.8765 - val_loss: -0.7553\n",
      "\n",
      "Epoch 00005: val_loss improved from -0.74280 to -0.75533, saving model to last2222.h5\n",
      "Epoch 6/200\n",
      "13800/13800 [==============================] - 989s 72ms/step - loss: -0.8964 - val_loss: -0.7790\n",
      "\n",
      "Epoch 00006: val_loss improved from -0.75533 to -0.77896, saving model to last2222.h5\n",
      "Epoch 7/200\n",
      "13800/13800 [==============================] - 985s 71ms/step - loss: -0.9125 - val_loss: -0.7848\n",
      "\n",
      "Epoch 00007: val_loss improved from -0.77896 to -0.78475, saving model to last2222.h5\n",
      "Epoch 8/200\n",
      "13800/13800 [==============================] - 983s 71ms/step - loss: -0.9120 - val_loss: -0.7748\n",
      "\n",
      "Epoch 00008: val_loss did not improve from -0.78475\n",
      "Epoch 9/200\n",
      "13800/13800 [==============================] - 984s 71ms/step - loss: -0.9354 - val_loss: -0.7647\n",
      "\n",
      "Epoch 00009: val_loss did not improve from -0.78475\n",
      "Epoch 10/200\n",
      "13800/13800 [==============================] - 984s 71ms/step - loss: -0.9403 - val_loss: -0.7814\n",
      "\n",
      "Epoch 00010: val_loss did not improve from -0.78475\n",
      "Epoch 11/200\n",
      "13800/13800 [==============================] - 983s 71ms/step - loss: -0.9528 - val_loss: -0.8018\n",
      "\n",
      "Epoch 00011: val_loss improved from -0.78475 to -0.80183, saving model to last2222.h5\n",
      "Epoch 12/200\n",
      "13800/13800 [==============================] - 983s 71ms/step - loss: -0.9537 - val_loss: -0.8039\n",
      "\n",
      "Epoch 00012: val_loss improved from -0.80183 to -0.80394, saving model to last2222.h5\n",
      "Epoch 13/200\n",
      "13800/13800 [==============================] - 982s 71ms/step - loss: -0.9606 - val_loss: -0.7954\n",
      "\n",
      "Epoch 00013: val_loss did not improve from -0.80394\n",
      "Epoch 14/200\n",
      "13800/13800 [==============================] - 984s 71ms/step - loss: -0.9688 - val_loss: -0.8080\n",
      "\n",
      "Epoch 00014: val_loss improved from -0.80394 to -0.80803, saving model to last2222.h5\n",
      "Epoch 15/200\n",
      "13800/13800 [==============================] - 985s 71ms/step - loss: -0.9723 - val_loss: -0.8222\n",
      "\n",
      "Epoch 00015: val_loss improved from -0.80803 to -0.82223, saving model to last2222.h5\n",
      "Epoch 16/200\n",
      "13800/13800 [==============================] - 984s 71ms/step - loss: -0.9754 - val_loss: -0.8099\n",
      "\n",
      "Epoch 00016: val_loss did not improve from -0.82223\n",
      "Epoch 17/200\n",
      "13800/13800 [==============================] - 984s 71ms/step - loss: -0.9780 - val_loss: -0.8055\n",
      "\n",
      "Epoch 00017: val_loss did not improve from -0.82223\n",
      "Epoch 18/200\n",
      "13800/13800 [==============================] - 984s 71ms/step - loss: -0.9798 - val_loss: -0.8110\n",
      "\n",
      "Epoch 00018: val_loss did not improve from -0.82223\n",
      "Epoch 19/200\n",
      "13800/13800 [==============================] - 984s 71ms/step - loss: -0.9814 - val_loss: -0.8282\n",
      "\n",
      "Epoch 00019: val_loss improved from -0.82223 to -0.82824, saving model to last2222.h5\n",
      "Epoch 20/200\n",
      "13800/13800 [==============================] - 982s 71ms/step - loss: -0.9756 - val_loss: -0.6493\n",
      "\n",
      "Epoch 00020: val_loss did not improve from -0.82824\n",
      "Epoch 21/200\n",
      "13800/13800 [==============================] - 976s 71ms/step - loss: -0.9743 - val_loss: -0.8161\n",
      "\n",
      "Epoch 00021: val_loss did not improve from -0.82824\n",
      "Epoch 22/200\n",
      "13800/13800 [==============================] - 976s 71ms/step - loss: -0.9871 - val_loss: -0.8168\n",
      "\n",
      "Epoch 00022: val_loss did not improve from -0.82824\n",
      "Epoch 23/200\n",
      "13800/13800 [==============================] - 970s 70ms/step - loss: -0.9886 - val_loss: -0.8246\n",
      "\n",
      "Epoch 00023: val_loss did not improve from -0.82824\n",
      "Epoch 24/200\n",
      "13800/13800 [==============================] - 958s 69ms/step - loss: -0.9894 - val_loss: -0.8130\n",
      "\n",
      "Epoch 00024: val_loss did not improve from -0.82824\n",
      "Epoch 25/200\n",
      "13800/13800 [==============================] - 948s 69ms/step - loss: -0.9901 - val_loss: -0.8214\n",
      "\n",
      "Epoch 00025: val_loss did not improve from -0.82824\n",
      "Epoch 26/200\n",
      "13800/13800 [==============================] - 977s 71ms/step - loss: -0.9905 - val_loss: -0.8290\n",
      "\n",
      "Epoch 00026: val_loss improved from -0.82824 to -0.82901, saving model to last2222.h5\n",
      "Epoch 27/200\n",
      "13800/13800 [==============================] - 975s 71ms/step - loss: -0.9905 - val_loss: -0.8195\n",
      "\n",
      "Epoch 00027: val_loss did not improve from -0.82901\n",
      "Epoch 28/200\n",
      "13800/13800 [==============================] - 952s 69ms/step - loss: -0.9893 - val_loss: -0.8084\n",
      "\n",
      "Epoch 00028: val_loss did not improve from -0.82901\n",
      "Epoch 29/200\n",
      "13800/13800 [==============================] - 951s 69ms/step - loss: -0.9869 - val_loss: -0.8143\n",
      "\n",
      "Epoch 00029: val_loss did not improve from -0.82901\n",
      "Epoch 30/200\n",
      "13800/13800 [==============================] - 928s 67ms/step - loss: -0.9928 - val_loss: -0.8123\n",
      "\n",
      "Epoch 00030: val_loss did not improve from -0.82901\n",
      "Epoch 31/200\n",
      "13800/13800 [==============================] - 931s 67ms/step - loss: -0.9932 - val_loss: -0.8094\n",
      "\n",
      "Epoch 00031: val_loss did not improve from -0.82901\n",
      "Epoch 32/200\n",
      "13800/13800 [==============================] - 938s 68ms/step - loss: -0.9935 - val_loss: -0.8130\n",
      "\n",
      "Epoch 00032: val_loss did not improve from -0.82901\n",
      "Epoch 33/200\n",
      "13800/13800 [==============================] - 935s 68ms/step - loss: -0.9937 - val_loss: -0.8129\n",
      "\n",
      "Epoch 00033: val_loss did not improve from -0.82901\n",
      "Epoch 34/200\n",
      "13800/13800 [==============================] - 967s 70ms/step - loss: -0.9936 - val_loss: -0.8142\n",
      "\n",
      "Epoch 00034: val_loss did not improve from -0.82901\n",
      "Epoch 35/200\n",
      "13800/13800 [==============================] - 942s 68ms/step - loss: -0.9940 - val_loss: -0.8227\n",
      "\n",
      "Epoch 00035: val_loss did not improve from -0.82901\n",
      "Epoch 36/200\n",
      "13800/13800 [==============================] - 934s 68ms/step - loss: -0.9941 - val_loss: -0.8240\n",
      "\n",
      "Epoch 00036: val_loss did not improve from -0.82901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd2cbe2aba8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet.fit([train_img,train_flo,train_rflo], train_seg,\n",
    "        shuffle=True,\n",
    "        epochs=200,\n",
    "        batch_size=20,\n",
    "        validation_data=([test_img,test_flo,test_rflo],test_seg),callbacks=[EarlyStopping,checkpoint,csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

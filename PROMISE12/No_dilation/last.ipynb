{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsun/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras import optimizers\n",
    "import keras.layers.advanced_activations\n",
    "import scipy\n",
    "import random\n",
    "import matplotlib.pyplot as plt  \n",
    "from scipy.stats import norm  \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import VarianceScaling,RandomNormal\n",
    "from keras.layers import Input, Dense, Dropout, Conv2D, MaxPooling2D, concatenate\n",
    "from keras.layers import UpSampling2D, Reshape, Lambda, Flatten, Activation,Concatenate,Add\n",
    "from keras.models import Model  \n",
    "from keras.optimizers import SGD, Adadelta, Adagrad,Adam\n",
    "from keras import backend as K  \n",
    "from keras import objectives  \n",
    "from keras.utils.vis_utils import plot_model  \n",
    "from keras.utils import np_utils, generic_utils\n",
    "import sys \n",
    "import tensorflow as tf\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "import sklearn\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import NullFormatter\n",
    "%matplotlib inline\n",
    "from sklearn import manifold, datasets,cluster\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = np.load(\"../train_img_aug.npy\").reshape([-1,256,256,1])\n",
    "train_seg = np.load(\"../train_seg_aug.npy\").reshape([-1,256,256,1])\n",
    "train_flo = np.load(\"../train_flo_aug.npy\")\n",
    "train_rflo = np.load(\"../train_rflo_aug.npy\")\n",
    "#train_img = np.log(train_img+1.0)\n",
    "train_seg = np_utils.to_categorical(train_seg, 2)\n",
    "\n",
    "test_img = np.load(\"../test_img.npy\").reshape([-1,256,256,1])\n",
    "test_seg = np.load(\"../test_seg.npy\").reshape([-1,256,256,1])\n",
    "test_flo = np.load(\"../test_flo.npy\")\n",
    "test_rflo = np.load(\"../test_rflo.npy\")\n",
    "test_seg = np_utils.to_categorical(test_seg, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsun/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:57: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"co...)`\n"
     ]
    }
   ],
   "source": [
    "img_rows, img_cols = 256, 256\n",
    "img_channels = 1\n",
    "\n",
    "batch_size =50\n",
    "latent_dim = 256\n",
    "nb_epoch = 50\n",
    "intermediate_dim =256\n",
    "original_dim = img_rows*img_cols\n",
    "LRelu = 'relu'\n",
    "\n",
    "#USE = 'autoencoder'\n",
    "#USE = 'vae'\n",
    "#encoder:\n",
    "input_img = Input(shape=(img_rows, img_cols,img_channels))\n",
    "input_flo = Input(shape=(img_rows, img_cols,3))\n",
    "input_rflo = Input(shape=(img_rows, img_cols,3))\n",
    "conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(input_img)\n",
    "conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "drop4 = Dropout(0.5)(conv4)\n",
    "pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "merge6 = Concatenate()([drop4,up6])\n",
    "conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "merge7 = Concatenate()([conv3,up7])\n",
    "conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "merge8 =Concatenate()([conv2,up8])\n",
    "conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "merge9 = Concatenate()([conv1,up9,input_flo,input_rflo])\n",
    "conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "conv10 = Conv2D(2, 1, activation = 'softmax')(conv9)\n",
    "\n",
    "unet = Model(input = [input_img,input_flo,input_rflo], output = conv10)\n",
    "\n",
    "#lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
    "#early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=7, verbose=0, mode='auto')\n",
    "csv_logger = CSVLogger('last.csv')\n",
    "\n",
    "#decoded = Conv2D(2, (3, 3), activation='softmax', padding='same')(upsample_10)\n",
    "#decoded = Flatten()(decoded)\n",
    "#decoded = Dense(256*256,activation='softmax')(decoded)\n",
    "\n",
    "\n",
    "EarlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')\n",
    "checkpoint = ModelCheckpoint('last.h5',monitor ='val_loss',verbose = 1,save_best_only = True)\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true[:,:,:,1]) \n",
    "    y_pred_f = K.flatten(y_pred[:,:,:,1]) \n",
    "    intersection = K.sum(y_true_f * y_pred_f) \n",
    "    return (2. * intersection + 1e-6) / (K.sum(K.square(y_true_f)) + K.sum(K.square(y_pred_f)) + 1e-6)\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "#def ae_loss(x, decoded):  \n",
    "#    xent_loss = original_dim * objectives.mean_squared_error(x,decoded)\n",
    "#    return xent_loss\n",
    "adam = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "#autoencoder = Model(inputs=input_img, outputs=decoded)\n",
    "unet.compile(optimizer=adam, loss=dice_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13800 samples, validate on 127 samples\n",
      "Epoch 1/200\n",
      "13800/13800 [==============================] - 914s 66ms/step - loss: -0.4533 - val_loss: -0.4953\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.49532, saving model to last.h5\n",
      "Epoch 2/200\n",
      "13800/13800 [==============================] - 910s 66ms/step - loss: -0.6285 - val_loss: -0.6830\n",
      "\n",
      "Epoch 00002: val_loss improved from -0.49532 to -0.68304, saving model to last.h5\n",
      "Epoch 3/200\n",
      "13800/13800 [==============================] - 907s 66ms/step - loss: -0.7708 - val_loss: -0.7368\n",
      "\n",
      "Epoch 00003: val_loss improved from -0.68304 to -0.73684, saving model to last.h5\n",
      "Epoch 4/200\n",
      "13800/13800 [==============================] - 908s 66ms/step - loss: -0.8358 - val_loss: -0.7530\n",
      "\n",
      "Epoch 00004: val_loss improved from -0.73684 to -0.75299, saving model to last.h5\n",
      "Epoch 5/200\n",
      "13800/13800 [==============================] - 907s 66ms/step - loss: -0.8655 - val_loss: -0.7718\n",
      "\n",
      "Epoch 00005: val_loss improved from -0.75299 to -0.77179, saving model to last.h5\n",
      "Epoch 6/200\n",
      "13800/13800 [==============================] - 907s 66ms/step - loss: -0.8825 - val_loss: -0.7804\n",
      "\n",
      "Epoch 00006: val_loss improved from -0.77179 to -0.78035, saving model to last.h5\n",
      "Epoch 7/200\n",
      "13800/13800 [==============================] - 906s 66ms/step - loss: -0.9079 - val_loss: -0.7793\n",
      "\n",
      "Epoch 00007: val_loss did not improve from -0.78035\n",
      "Epoch 8/200\n",
      "13800/13800 [==============================] - 903s 65ms/step - loss: -0.9236 - val_loss: -0.7671\n",
      "\n",
      "Epoch 00008: val_loss did not improve from -0.78035\n",
      "Epoch 9/200\n",
      "13800/13800 [==============================] - 903s 65ms/step - loss: -0.9331 - val_loss: -0.8114\n",
      "\n",
      "Epoch 00009: val_loss improved from -0.78035 to -0.81137, saving model to last.h5\n",
      "Epoch 10/200\n",
      "13800/13800 [==============================] - 903s 65ms/step - loss: -0.9437 - val_loss: -0.7944\n",
      "\n",
      "Epoch 00010: val_loss did not improve from -0.81137\n",
      "Epoch 11/200\n",
      "13800/13800 [==============================] - 903s 65ms/step - loss: -0.9513 - val_loss: -0.8087\n",
      "\n",
      "Epoch 00011: val_loss did not improve from -0.81137\n",
      "Epoch 12/200\n",
      "13800/13800 [==============================] - 901s 65ms/step - loss: -0.9428 - val_loss: -0.7928\n",
      "\n",
      "Epoch 00012: val_loss did not improve from -0.81137\n",
      "Epoch 13/200\n",
      "13800/13800 [==============================] - 900s 65ms/step - loss: -0.9628 - val_loss: -0.8022\n",
      "\n",
      "Epoch 00013: val_loss did not improve from -0.81137\n",
      "Epoch 14/200\n",
      "13800/13800 [==============================] - 901s 65ms/step - loss: -0.9686 - val_loss: -0.8145\n",
      "\n",
      "Epoch 00014: val_loss improved from -0.81137 to -0.81449, saving model to last.h5\n",
      "Epoch 15/200\n",
      "13800/13800 [==============================] - 901s 65ms/step - loss: -0.9721 - val_loss: -0.8192\n",
      "\n",
      "Epoch 00015: val_loss improved from -0.81449 to -0.81920, saving model to last.h5\n",
      "Epoch 16/200\n",
      "13800/13800 [==============================] - 901s 65ms/step - loss: -0.9756 - val_loss: -0.8024\n",
      "\n",
      "Epoch 00016: val_loss did not improve from -0.81920\n",
      "Epoch 17/200\n",
      "13800/13800 [==============================] - 902s 65ms/step - loss: -0.9780 - val_loss: -0.7878\n",
      "\n",
      "Epoch 00017: val_loss did not improve from -0.81920\n",
      "Epoch 18/200\n",
      "13800/13800 [==============================] - 902s 65ms/step - loss: -0.9799 - val_loss: -0.7981\n",
      "\n",
      "Epoch 00018: val_loss did not improve from -0.81920\n",
      "Epoch 19/200\n",
      "13800/13800 [==============================] - 899s 65ms/step - loss: -0.9706 - val_loss: -0.8258\n",
      "\n",
      "Epoch 00019: val_loss improved from -0.81920 to -0.82581, saving model to last.h5\n",
      "Epoch 20/200\n",
      "13800/13800 [==============================] - 899s 65ms/step - loss: -0.9845 - val_loss: -0.8218\n",
      "\n",
      "Epoch 00020: val_loss did not improve from -0.82581\n",
      "Epoch 21/200\n",
      "13800/13800 [==============================] - 900s 65ms/step - loss: -0.9865 - val_loss: -0.8194\n",
      "\n",
      "Epoch 00021: val_loss did not improve from -0.82581\n",
      "Epoch 22/200\n",
      "13800/13800 [==============================] - 900s 65ms/step - loss: -0.9873 - val_loss: -0.8031\n",
      "\n",
      "Epoch 00022: val_loss did not improve from -0.82581\n",
      "Epoch 23/200\n",
      "13800/13800 [==============================] - 897s 65ms/step - loss: -0.9884 - val_loss: -0.8208\n",
      "\n",
      "Epoch 00023: val_loss did not improve from -0.82581\n",
      "Epoch 24/200\n",
      "13800/13800 [==============================] - 895s 65ms/step - loss: -0.9889 - val_loss: -0.8266\n",
      "\n",
      "Epoch 00024: val_loss improved from -0.82581 to -0.82662, saving model to last.h5\n",
      "Epoch 25/200\n",
      "13800/13800 [==============================] - 893s 65ms/step - loss: -0.9899 - val_loss: -0.8370\n",
      "\n",
      "Epoch 00025: val_loss improved from -0.82662 to -0.83695, saving model to last.h5\n",
      "Epoch 26/200\n",
      "13800/13800 [==============================] - 888s 64ms/step - loss: -0.9823 - val_loss: -0.8316\n",
      "\n",
      "Epoch 00026: val_loss did not improve from -0.83695\n",
      "Epoch 27/200\n",
      "13800/13800 [==============================] - 861s 62ms/step - loss: -0.9881 - val_loss: -0.8212\n",
      "\n",
      "Epoch 00027: val_loss did not improve from -0.83695\n",
      "Epoch 28/200\n",
      "13800/13800 [==============================] - 886s 64ms/step - loss: -0.9921 - val_loss: -0.8280\n",
      "\n",
      "Epoch 00028: val_loss did not improve from -0.83695\n",
      "Epoch 29/200\n",
      "13800/13800 [==============================] - 896s 65ms/step - loss: -0.9926 - val_loss: -0.8201\n",
      "\n",
      "Epoch 00029: val_loss did not improve from -0.83695\n",
      "Epoch 30/200\n",
      "13800/13800 [==============================] - 885s 64ms/step - loss: -0.9930 - val_loss: -0.8211\n",
      "\n",
      "Epoch 00030: val_loss did not improve from -0.83695\n",
      "Epoch 31/200\n",
      "13800/13800 [==============================] - 886s 64ms/step - loss: -0.9931 - val_loss: -0.8300\n",
      "\n",
      "Epoch 00031: val_loss did not improve from -0.83695\n",
      "Epoch 32/200\n",
      "13800/13800 [==============================] - 900s 65ms/step - loss: -0.9926 - val_loss: -0.7891\n",
      "\n",
      "Epoch 00032: val_loss did not improve from -0.83695\n",
      "Epoch 33/200\n",
      "13800/13800 [==============================] - 887s 64ms/step - loss: -0.9862 - val_loss: -0.8334\n",
      "\n",
      "Epoch 00033: val_loss did not improve from -0.83695\n",
      "Epoch 34/200\n",
      "13800/13800 [==============================] - 884s 64ms/step - loss: -0.9940 - val_loss: -0.8289\n",
      "\n",
      "Epoch 00034: val_loss did not improve from -0.83695\n",
      "Epoch 35/200\n",
      "13800/13800 [==============================] - 881s 64ms/step - loss: -0.9944 - val_loss: -0.8314\n",
      "\n",
      "Epoch 00035: val_loss did not improve from -0.83695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9a303ba0f0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet.fit([train_img,train_flo,train_rflo], train_seg,\n",
    "        shuffle=True,\n",
    "        epochs=200,\n",
    "        batch_size=20,\n",
    "        validation_data=([test_img,test_flo,test_rflo],test_seg),callbacks=[EarlyStopping,checkpoint,csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsun/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras import optimizers\n",
    "import keras.layers.advanced_activations\n",
    "import scipy\n",
    "import random\n",
    "import matplotlib.pyplot as plt  \n",
    "from scipy.stats import norm  \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import VarianceScaling,RandomNormal\n",
    "from keras.layers import Input, Dense, Dropout, Conv2D, MaxPooling2D, concatenate\n",
    "from keras.layers import UpSampling2D, Reshape, Lambda, Flatten, Activation,Concatenate,Add\n",
    "from keras.models import Model  \n",
    "from keras.optimizers import SGD, Adadelta, Adagrad,Adam\n",
    "from keras import backend as K  \n",
    "from keras import objectives  \n",
    "from keras.utils.vis_utils import plot_model  \n",
    "from keras.utils import np_utils, generic_utils\n",
    "import sys \n",
    "import tensorflow as tf\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import sklearn\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import NullFormatter\n",
    "%matplotlib inline\n",
    "from sklearn import manifold, datasets,cluster\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = np.load(\"../train_img_aug.npy\").reshape([-1,256,256,1])\n",
    "train_seg = np.load(\"../train_seg_aug.npy\").reshape([-1,256,256,1])\n",
    "#train_flo = np.load(\"../train_flo_aug.npy\")\n",
    "#train_rflo = np.load(\"../train_rflo_aug.npy\")\n",
    "#train_img = np.log(train_img+1.0)\n",
    "train_seg = np_utils.to_categorical(train_seg, 2)\n",
    "\n",
    "test_img = np.load(\"../test_img.npy\").reshape([-1,256,256,1])\n",
    "test_seg = np.load(\"../test_seg.npy\").reshape([-1,256,256,1])\n",
    "#test_flo = np.load(\"../test_flo.npy\")\n",
    "#test_rflo = np.load(\"../test_rflo.npy\")\n",
    "test_seg = np_utils.to_categorical(test_seg, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsun/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:56: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"
     ]
    }
   ],
   "source": [
    "img_rows, img_cols = 256, 256\n",
    "img_channels = 1\n",
    "\n",
    "batch_size =50\n",
    "latent_dim = 256\n",
    "nb_epoch = 50\n",
    "intermediate_dim =256\n",
    "original_dim = img_rows*img_cols\n",
    "LRelu = 'relu'\n",
    "\n",
    "#USE = 'autoencoder'\n",
    "#USE = 'vae'\n",
    "#encoder:\n",
    "input_img = Input(shape=(img_rows, img_cols,img_channels))\n",
    "\n",
    "conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(input_img)\n",
    "conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "drop4 = Dropout(0.5)(conv4)\n",
    "pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "merge6 = Concatenate()([drop4,up6])\n",
    "conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "merge7 = Concatenate()([conv3,up7])\n",
    "conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "merge8 =Concatenate()([conv2,up8])\n",
    "conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "merge9 = Concatenate()([conv1,up9])\n",
    "conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "conv10 = Conv2D(2, 1, activation = 'softmax')(conv9)\n",
    "\n",
    "unet = Model(input = input_img, output = conv10)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
    "#early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=7, verbose=0, mode='auto')\n",
    "csv_logger = CSVLogger('baseline_no.csv')\n",
    "\n",
    "\n",
    "\n",
    "#decoded = Conv2D(2, (3, 3), activation='softmax', padding='same')(upsample_10)\n",
    "#decoded = Flatten()(decoded)\n",
    "#decoded = Dense(256*256,activation='softmax')(decoded)\n",
    "\n",
    "\n",
    "EarlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')\n",
    "checkpoint = ModelCheckpoint('baseline_no.h5',monitor ='val_loss',verbose = 1,save_best_only = True)\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true[:,:,:,1]) \n",
    "    y_pred_f = K.flatten(y_pred[:,:,:,1]) \n",
    "    intersection = K.sum(y_true_f * y_pred_f) \n",
    "    return (2. * intersection + 1e-6) / (K.sum(K.square(y_true_f)) + K.sum(K.square(y_pred_f)) + 1e-6)\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "#def ae_loss(x, decoded):  \n",
    "#    xent_loss = original_dim * objectives.mean_squared_error(x,decoded)\n",
    "#    return xent_loss\n",
    "adam = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "#autoencoder = Model(inputs=input_img, outputs=decoded)\n",
    "unet.compile(optimizer=adam, loss=dice_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13800 samples, validate on 127 samples\n",
      "Epoch 1/200\n",
      "13800/13800 [==============================] - 856s 62ms/step - loss: -0.5098 - val_loss: -0.5385\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.53847, saving model to baseline_no.h5\n",
      "Epoch 2/200\n",
      "13800/13800 [==============================] - 851s 62ms/step - loss: -0.6897 - val_loss: -0.7084\n",
      "\n",
      "Epoch 00002: val_loss improved from -0.53847 to -0.70836, saving model to baseline_no.h5\n",
      "Epoch 3/200\n",
      "13800/13800 [==============================] - 851s 62ms/step - loss: -0.7864 - val_loss: -0.7407\n",
      "\n",
      "Epoch 00003: val_loss improved from -0.70836 to -0.74067, saving model to baseline_no.h5\n",
      "Epoch 4/200\n",
      "13800/13800 [==============================] - 851s 62ms/step - loss: -0.8405 - val_loss: -0.7518\n",
      "\n",
      "Epoch 00004: val_loss improved from -0.74067 to -0.75182, saving model to baseline_no.h5\n",
      "Epoch 5/200\n",
      "13800/13800 [==============================] - 851s 62ms/step - loss: -0.8723 - val_loss: -0.7803\n",
      "\n",
      "Epoch 00005: val_loss improved from -0.75182 to -0.78030, saving model to baseline_no.h5\n",
      "Epoch 6/200\n",
      "13800/13800 [==============================] - 850s 62ms/step - loss: -0.8894 - val_loss: -0.7911\n",
      "\n",
      "Epoch 00006: val_loss improved from -0.78030 to -0.79114, saving model to baseline_no.h5\n",
      "Epoch 7/200\n",
      "13800/13800 [==============================] - 850s 62ms/step - loss: -0.9126 - val_loss: -0.7835\n",
      "\n",
      "Epoch 00007: val_loss did not improve from -0.79114\n",
      "Epoch 8/200\n",
      "13800/13800 [==============================] - 850s 62ms/step - loss: -0.9228 - val_loss: -0.7624\n",
      "\n",
      "Epoch 00008: val_loss did not improve from -0.79114\n",
      "Epoch 9/200\n",
      "13800/13800 [==============================] - 849s 62ms/step - loss: -0.9272 - val_loss: -0.7868\n",
      "\n",
      "Epoch 00009: val_loss did not improve from -0.79114\n",
      "Epoch 10/200\n",
      "13800/13800 [==============================] - 850s 62ms/step - loss: -0.9451 - val_loss: -0.7959\n",
      "\n",
      "Epoch 00010: val_loss improved from -0.79114 to -0.79588, saving model to baseline_no.h5\n",
      "Epoch 11/200\n",
      "13800/13800 [==============================] - 849s 62ms/step - loss: -0.9527 - val_loss: -0.7926\n",
      "\n",
      "Epoch 00011: val_loss did not improve from -0.79588\n",
      "Epoch 12/200\n",
      "13800/13800 [==============================] - 850s 62ms/step - loss: -0.9563 - val_loss: -0.8064\n",
      "\n",
      "Epoch 00012: val_loss improved from -0.79588 to -0.80642, saving model to baseline_no.h5\n",
      "Epoch 13/200\n",
      "13800/13800 [==============================] - 847s 61ms/step - loss: -0.9546 - val_loss: -0.7913\n",
      "\n",
      "Epoch 00013: val_loss did not improve from -0.80642\n",
      "Epoch 14/200\n",
      "13800/13800 [==============================] - 849s 62ms/step - loss: -0.9693 - val_loss: -0.8114\n",
      "\n",
      "Epoch 00014: val_loss improved from -0.80642 to -0.81142, saving model to baseline_no.h5\n",
      "Epoch 15/200\n",
      "13800/13800 [==============================] - 849s 61ms/step - loss: -0.9733 - val_loss: -0.8176\n",
      "\n",
      "Epoch 00015: val_loss improved from -0.81142 to -0.81755, saving model to baseline_no.h5\n",
      "Epoch 16/200\n",
      "13800/13800 [==============================] - 849s 62ms/step - loss: -0.9763 - val_loss: -0.8216\n",
      "\n",
      "Epoch 00016: val_loss improved from -0.81755 to -0.82160, saving model to baseline_no.h5\n",
      "Epoch 17/200\n",
      "13800/13800 [==============================] - 848s 61ms/step - loss: -0.9558 - val_loss: -0.7970\n",
      "\n",
      "Epoch 00017: val_loss did not improve from -0.82160\n",
      "Epoch 18/200\n",
      "13800/13800 [==============================] - 847s 61ms/step - loss: -0.9802 - val_loss: -0.8094\n",
      "\n",
      "Epoch 00018: val_loss did not improve from -0.82160\n",
      "Epoch 19/200\n",
      "13800/13800 [==============================] - 848s 61ms/step - loss: -0.9835 - val_loss: -0.8139\n",
      "\n",
      "Epoch 00019: val_loss did not improve from -0.82160\n",
      "Epoch 20/200\n",
      "13800/13800 [==============================] - 848s 61ms/step - loss: -0.9851 - val_loss: -0.8139\n",
      "\n",
      "Epoch 00020: val_loss did not improve from -0.82160\n",
      "Epoch 21/200\n",
      "13800/13800 [==============================] - 848s 61ms/step - loss: -0.9864 - val_loss: -0.8230\n",
      "\n",
      "Epoch 00021: val_loss improved from -0.82160 to -0.82305, saving model to baseline_no.h5\n",
      "Epoch 22/200\n",
      "13800/13800 [==============================] - 848s 61ms/step - loss: -0.9876 - val_loss: -0.8207\n",
      "\n",
      "Epoch 00022: val_loss did not improve from -0.82305\n",
      "Epoch 23/200\n",
      "13800/13800 [==============================] - 847s 61ms/step - loss: -0.9741 - val_loss: -0.8274\n",
      "\n",
      "Epoch 00023: val_loss improved from -0.82305 to -0.82745, saving model to baseline_no.h5\n",
      "Epoch 24/200\n",
      "13800/13800 [==============================] - 817s 59ms/step - loss: -0.9875 - val_loss: -0.8136\n",
      "\n",
      "Epoch 00024: val_loss did not improve from -0.82745\n",
      "Epoch 25/200\n",
      "13800/13800 [==============================] - 811s 59ms/step - loss: -0.9905 - val_loss: -0.8114\n",
      "\n",
      "Epoch 00025: val_loss did not improve from -0.82745\n",
      "Epoch 26/200\n",
      "13800/13800 [==============================] - 809s 59ms/step - loss: -0.9913 - val_loss: -0.8169\n",
      "\n",
      "Epoch 00026: val_loss did not improve from -0.82745\n",
      "Epoch 27/200\n",
      "13800/13800 [==============================] - 817s 59ms/step - loss: -0.9918 - val_loss: -0.8172\n",
      "\n",
      "Epoch 00027: val_loss did not improve from -0.82745\n",
      "Epoch 28/200\n",
      "13800/13800 [==============================] - 812s 59ms/step - loss: -0.9921 - val_loss: -0.8263\n",
      "\n",
      "Epoch 00028: val_loss did not improve from -0.82745\n",
      "Epoch 29/200\n",
      "13800/13800 [==============================] - 816s 59ms/step - loss: -0.9938 - val_loss: -0.8241\n",
      "\n",
      "Epoch 00029: val_loss did not improve from -0.82745\n",
      "Epoch 30/200\n",
      "13800/13800 [==============================] - 812s 59ms/step - loss: -0.9943 - val_loss: -0.8213\n",
      "\n",
      "Epoch 00030: val_loss did not improve from -0.82745\n",
      "Epoch 31/200\n",
      "13800/13800 [==============================] - 816s 59ms/step - loss: -0.9946 - val_loss: -0.8228\n",
      "\n",
      "Epoch 00031: val_loss did not improve from -0.82745\n",
      "Epoch 32/200\n",
      "13800/13800 [==============================] - 816s 59ms/step - loss: -0.9947 - val_loss: -0.8200\n",
      "\n",
      "Epoch 00032: val_loss did not improve from -0.82745\n",
      "Epoch 33/200\n",
      "13800/13800 [==============================] - 810s 59ms/step - loss: -0.9949 - val_loss: -0.8206\n",
      "\n",
      "Epoch 00033: val_loss did not improve from -0.82745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f040c12c400>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet.fit(train_img, train_seg,\n",
    "        shuffle=True,\n",
    "        epochs=200,\n",
    "        batch_size=20,\n",
    "        validation_data=(test_img,test_seg),callbacks=[EarlyStopping,checkpoint,csv_logger,lr_reducer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
